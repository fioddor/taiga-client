[{"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109025, "slug": "all-pages", "content": "", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-22T09:48:07Z", "modified_date": "2017-11-22T09:48:07Z", "html": "", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109026, "slug": "architecture", "content": "# eLinks Architecture\n\n![](https://media.taiga.io/attachments/0/e/5/1/5c98d25d14ad47ca9d1adb7ce4468a178dae8fc9f6157d92cd2c8ca035e5/searchandindexoverview.png)\n\n![](https://media.taiga.io/attachments/7/d/f/4/00b648d6592384722c77b6fafed6b12de704958f464f6ed1bd43640a5571/updateoverview.png)\n\n![](https://media.taiga.io/attachments/3/1/1/f/c4d7d839463935a6b0216132f0f16ab871fd19d282191436ba4e92baaf2c/authoritytemplate.png)", "owner": 133564, "last_modifier": 133564, "created_date": "2018-06-18T13:32:01Z", "modified_date": "2018-06-18T13:39:55Z", "html": "<h1 id=\"elinks-architecture\">eLinks Architecture</h1>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/0/e/5/1/5c98d25d14ad47ca9d1adb7ce4468a178dae8fc9f6157d92cd2c8ca035e5/searchandindexoverview.png\"></p>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/7/d/f/4/00b648d6592384722c77b6fafed6b12de704958f464f6ed1bd43640a5571/updateoverview.png\"></p>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/3/1/1/f/c4d7d839463935a6b0216132f0f16ab871fd19d282191436ba4e92baaf2c/authoritytemplate.png\"></p>", "editions": 11, "version": 5}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109027, "slug": "code-repo", "content": "# eLINKS Repository\n\n## Accessing the repository\n\nThe code repository is hosted on bitbucket [here](https://bitbucket.org/alastairduncan/linker). To gain access, you'll need to first create a bitbucket account using your STFC email address, then ask Alastair or Alan to add you to the project.\n\n## Automatic Status Updates\n\nIt has been webhooks linked with this project so that when code is pushed to the repository appropriately formatted comments can be used to move tasks to an appropriate status.\n\nTaiga can listen for three triggers. Currently, only push is enabled, but the other triggers could be implemented if needed.\n\n*   Repository - Push: Changing element status via commit message\n*   Issue - Created: To clone new issues created in Bitbucket to Taiga\n*   Issue - Comment created: To add new comments to related issues\n\nComments on commit should be in the form of TG-5 #closed This particular comment was used to close task 5.", "owner": 133564, "last_modifier": 150610, "created_date": "2016-05-06T10:25:55Z", "modified_date": "2017-12-11T06:40:00Z", "html": "<h1 id=\"elinks-repository\">eLINKS Repository</h1>\n<h2 id=\"accessing-the-repository\">Accessing the repository</h2>\n<p>The code repository is hosted on bitbucket <a href=\"https://bitbucket.org/alastairduncan/linker\" target=\"_blank\">here</a>. To gain access, you'll need to first create a bitbucket account using your STFC email address, then ask Alastair or Alan to add you to the project.</p>\n<h2 id=\"automatic-status-updates\">Automatic Status Updates</h2>\n<p>It has been webhooks linked with this project so that when code is pushed to the repository appropriately formatted comments can be used to move tasks to an appropriate status.</p>\n<p>Taiga can listen for three triggers. Currently, only push is enabled, but the other triggers could be implemented if needed.</p>\n<ul>\n<li>Repository - Push: Changing element status via commit message</li>\n<li>Issue - Created: To clone new issues created in Bitbucket to Taiga</li>\n<li>Issue - Comment created: To add new comments to related issues</li>\n</ul>\n<p>Comments on commit should be in the form of TG-5 #closed This particular comment was used to close task 5.</p>", "editions": 4, "version": 4}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109028, "slug": "daaas-setup", "content": "#ODAaaS setup via ansible\n\nThis also sets up a jupyter environment which can be used to demo the Jupyter plugin\n\nhttps://tree.taiga.io/project/brianritchie1312-ijp/wiki/ansible-vm-config", "owner": 133564, "last_modifier": 133564, "created_date": "2018-08-21T12:05:40Z", "modified_date": "2018-08-21T12:05:40Z", "html": "<h1 id=\"odaaas-setup-via-ansible\">ODAaaS setup via ansible</h1>\n<p>This also sets up a jupyter environment which can be used to demo the Jupyter plugin</p>\n<p><a href=\"https://tree.taiga.io/project/brianritchie1312-ijp/wiki/ansible-vm-config\">https://tree.taiga.io/project/brianritchie1312-ijp/wiki/ansible-vm-config</a></p>", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109029, "slug": "demo-notes", "content": "# How to demo THE PROJECT\n\nOne of the biggest foreseeable challenges to the project is getting users to engage with the tool, and establish an initial base of users. The project began with a vision of what data management could be, rather than researchers having a specific need for the tool. As a result, the project involves not only developing and maintaining the tool, but also convincing researchers to use it, so being able to demonstrate the tool effectively is important.\n\nHere are some rough outlines of how to demo the project:\n\n*   [How to demo eLinks](/project/alastairduncan-aggregator/wiki/elinks-demo)\n*   [How to demo Jupyter](/project/alastairduncan-aggregator/wiki/jupyter-demo)\n\nHere are my more general experiences with demonstrations so far:\n\n*   **Use specific examples:** Even if you explain how the tool works perfectly, audiences can struggle to engage unless you present them with a relevant example (ideally, to their work specifically).\n    It helps hugely if the paper and dataset are familiar to the researchers you are presenting to. It's easy to think that audiences will give you the benefit of the doubt, and follow an example using any generic paper and DOI combination, but they'll often get hung up on it if the science is wrong.\n\n*   **Explain the problems the tool can solve:** Before showing the tool, I found it was often helpful to outline exactly what the aim of the tool is. For example, for demoing eLinks, have two tabs open in the browser- one showing a paper, the other showing a related dataset. Explain the situation: a researcher has a published paper and dataset, and wants to make the dataset easier to find. eLinks can be used to create this link, and improve access to their data.\n\n*   **Be clear about what stage the project is at:** Help the audience to direct their feedback by giving a small amount of context for the project. Early on, you ideally want them to suggest new features, and be ambitious with their feedback, to help direct the rest of the project. If you have placeholder elements or unfinished aspects, be clear about this, otherwise you'll get a lot of unhelpful feedback. Later on, specific feedback is a lot more welcome, so make sure the audience knows that they are seeing something which is intended to be the finished product.\n\n*   **Be prepared for some resistance from researchers:** If what you're showing represents a potentially significant change to a researcher's workflow, they may be quite openly unenthusiastic about it. In particular, some researchers disliked the idea of publishing their data analysis, due to the possibility of other researchers building on their work and beating them to a major discovery. This is understandable, but remember that technology moves at a fast pace- something which is dismissed today could easily become the standard in two years' time.\n\n*   **Encourage interruptions:** If people keep interrupting to ask questions, definitely take the time to discuss them. It's a positive sign that the audience are taking an active interest. If possible, take regular breaks to give the audience chance to ask a question.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-23T12:10:54Z", "modified_date": "2017-11-23T13:59:24Z", "html": "<h1 id=\"how-to-demo-the-project\">How to demo THE PROJECT</h1>\n<p>One of the biggest foreseeable challenges to the project is getting users to engage with the tool, and establish an initial base of users. The project began with a vision of what data management could be, rather than researchers having a specific need for the tool. As a result, the project involves not only developing and maintaining the tool, but also convincing researchers to use it, so being able to demonstrate the tool effectively is important.</p>\n<p>Here are some rough outlines of how to demo the project:</p>\n<ul>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/elinks-demo\">How to demo eLinks</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-demo\">How to demo Jupyter</a></li>\n</ul>\n<p>Here are my more general experiences with demonstrations so far:</p>\n<ul>\n<li>\n<p><strong>Use specific examples:</strong> Even if you explain how the tool works perfectly, audiences can struggle to engage unless you present them with a relevant example (ideally, to their work specifically).<br>\n    It helps hugely if the paper and dataset are familiar to the researchers you are presenting to. It's easy to think that audiences will give you the benefit of the doubt, and follow an example using any generic paper and DOI combination, but they'll often get hung up on it if the science is wrong.</p>\n</li>\n<li>\n<p><strong>Explain the problems the tool can solve:</strong> Before showing the tool, I found it was often helpful to outline exactly what the aim of the tool is. For example, for demoing eLinks, have two tabs open in the browser- one showing a paper, the other showing a related dataset. Explain the situation: a researcher has a published paper and dataset, and wants to make the dataset easier to find. eLinks can be used to create this link, and improve access to their data.</p>\n</li>\n<li>\n<p><strong>Be clear about what stage the project is at:</strong> Help the audience to direct their feedback by giving a small amount of context for the project. Early on, you ideally want them to suggest new features, and be ambitious with their feedback, to help direct the rest of the project. If you have placeholder elements or unfinished aspects, be clear about this, otherwise you'll get a lot of unhelpful feedback. Later on, specific feedback is a lot more welcome, so make sure the audience knows that they are seeing something which is intended to be the finished product.</p>\n</li>\n<li>\n<p><strong>Be prepared for some resistance from researchers:</strong> If what you're showing represents a potentially significant change to a researcher's workflow, they may be quite openly unenthusiastic about it. In particular, some researchers disliked the idea of publishing their data analysis, due to the possibility of other researchers building on their work and beating them to a major discovery. This is understandable, but remember that technology moves at a fast pace- something which is dismissed today could easily become the standard in two years' time.</p>\n</li>\n<li>\n<p><strong>Encourage interruptions:</strong> If people keep interrupting to ask questions, definitely take the time to discuss them. It's a positive sign that the audience are taking an active interest. If possible, take regular breaks to give the audience chance to ask a question.</p>\n</li>\n</ul>", "editions": 3, "version": 3}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109030, "slug": "document-builder", "content": "# Document Builder #\n\nThe idea here is that this tool can be used to create a publication with the data, analysis software and tools along with the text which describes the methodology and techniques used, very much like an electronic lab book. The addition to this will be the ability to assemble a research object from the resources within the document and deposit these in a repository, populate the document with the references and publish the document as a preprint in a repository. ", "owner": 133564, "last_modifier": 133564, "created_date": "2016-05-06T09:26:21Z", "modified_date": "2016-05-06T09:26:21Z", "html": "<h1 id=\"document-builder\">Document Builder</h1>\n<p>The idea here is that this tool can be used to create a publication with the data, analysis software and tools along with the text which describes the methodology and techniques used, very much like an electronic lab book. The addition to this will be the ability to assemble a research object from the resources within the document and deposit these in a repository, populate the document with the references and publish the document as a preprint in a repository. </p>", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109031, "slug": "elinks-code", "content": "# eLinks: Code Structure\n\nThe code for eLinks can be split roughly into two sections- the frontend HTML, and the backend Java code.\n\n## Frontend Code\n\nThe frontend code can be found in the src/main/webapp directory. eLinks uses a framework called JSF (JavaServer Faces), which allows pages to be written in HTML, and make calls to the backend Java code. The webapp directory contains the pages to be run, the resources they require, and also the required JSF configuration.\n\nThe WEB-INTF subdirectory contains the configuration files. The important ones are:\n\n*   pretty-config.xml: This determines the pages available, and identifies any parameters available to the page. For example, the code below defines the link edit page. It will run the linkEditView class's load method when the page opens, and set the id parameter to be whatever is in the URL when navigating to the page. The page shown will be determined by the file /views/link-edit.xhtml.\n\n```\n<url-mapping id=\"link-edit\">\n    <pattern value=\"/link/#{linkEditView.id}/edit\" />\n    <view-id value=\"/views/link-edit.xhtml\" />\n    <action onPostback=\"false\">#{linkEditView.load}</action>\n</url-mapping>\n```\n\n*   web.xml: This contains most of the general config required to run the webapp. Most of this is unlikely to be changed, but you may need to add new listeners here if any other threads are added.\n*   faces-config.xml: This is used to register tools such as validators or converters with JSF.\n\nThe bulk of the code is found in the templates and views directories. The templates give the overall layouts of the pages, to ensure consistency. The views give the specific content of each page. Some content is repeated between pages, such as the login button, which appears on every page. These can be found in views/partials.\n\nThe resources directory contains all the other files needed by the webapp. This includes:\n\n*   Images, such as the logos, background, and loading animation.\n*   Sample licence files.\n*   CSS files: These determine the style of the webpage, such as setting font sizes and alignments of text.\n*   JavaScript files: These are simple scripts used by the site. They control features such as the show/hide button on the eData page.\n\n## Backend Code\n\nThe backend java code can be found in src/main/java/uk/ac/rl/aggregator. (Previously, the project was known as the aggregator, since it aggregates data and publications, but was rebranded as eLinks later in development. Ideally, we'd change the directory name, but it's probably not worth the trouble.)\n\nNote that Taiga automatically creates links when it detects certain extensions, such as .java. Please ignore these.\n\nThe code is split into directories, as follows.\n\n*   **Authority:** The authority classes provide an interface between Solr (the search engine) and the rest of the webapp. Each source of data or publication handled by eLinks has it's own authority class, which are handled in similar ways.\n    [Authority.java](http://Authority.java) provides a template for how each authority works, and an implementation of the template is provided for each source. Only one instance of each source authority will be running at any time, and the AuthorityFactory class is used to access that instance- for example, to get access to the ePubs authority, just call AuthorityFactory.createAuthority with an ePubs work as the argument, and the instance of the ePubs authority class will be returned.\n\n    For more details on how this works, see the 'singleton' and 'factory method' pages on [this](http://www.oodesign.com/) tutorial site.\n\n*   **Config:** This allows the webapp to access external configuration files. Config.java reads the file /opt/elinks/config/[config.properties](http://config.properties), and provides access to the fields that have been set there. The fields themselves are detailed in [Constants.java](http://Constants.java). Since the names of the fields may change over time, it's much safer to refer to the fields by the name given in the constants class, rather than using the actual name in the config file.\n\n*   **Content:** These modules relate to the help pages. They access the database to find help text, and allow users to edit and save the text on the pages.\n\n*   **Http:** These are just a series of wrappers around web requests.\n\n*   **Indexer**: The indexer classes are used to index new works and publications. When something new is added (for example, a new data item is manually linked to an ePubs record), the indexer needs to add the new data item to the search engine, but also update any related records with the new link. The indexer works similarly to the authorities, as each type of data has their own class which implements a template and is accessed by an IndexerFactory.\n    In addition to the indexer classes, this directory contains the indexer thread. See [here](elinks-multithreading) for more details.\n\n*   **Search:** These classes are used to search Solr, and manage the returned results.\n\n*   **Security:** This directory contains the authentication and sanitiser classes. The authenticator simply checks if a username's password is valid, and the sanitiser is used to check the input of text fields is legitimate.\n\n*   **Services:** This contains the DSpace URL linker, which searches ePubs for linked eData URLs which need a DOI to be added in. See [here](elinks-multithreading) for more details. _TODO: It might be better to move the other threads to this directory, and possibly rename it?_\n\n*   **UI:** This directory contains the bulk of the backing code. Certain classes have the line \"@ManagedBean(name = \"<class name>\")\" before the class definition- these are managed beans, which can be called from the HTML and provide the information the webapp needs to display.\n\n    Many of the pages have a class to act as the managed bean, which wraps an implementation class containing all the actual functionality (for example, [SearchView.java](http://SearchView.java) and [SearchViewImpl.java](http://SearchViewImpl.java)). The reason for this distinction is that the implementation class can be passed to the unit tests. The Java unit tests don't actually run the webapp itself, so the managed bean infrastructure doesn't work there. By making the implementation of the class available separately to the managed bean, the implementation can be tested with JUnit.\n\n    _Note: this directory is probably a bit too big. In particular, the link manager could potentially be moved to a different directory, and the dspaceCreate subdirectory should probably be it's own directory._\n\n## Test Code\n\nThe backend code is tested by JUnit, and the frontend code is tested by Selenium. Both sets of tests are available in the src/test directory. See the links below for advice on testing:\n\n*   [Testing with JUnit](/project/alastairduncan-aggregator/wiki/testing-with-junit)\n*   [Testing with Selenium](/project/alastairduncan-aggregator/wiki/testing-with-selenium)", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-04T15:45:56Z", "modified_date": "2017-12-11T09:43:26Z", "html": "<h1 id=\"elinks-code-structure\">eLinks: Code Structure</h1>\n<p>The code for eLinks can be split roughly into two sections- the frontend HTML, and the backend Java code.</p>\n<h2 id=\"frontend-code\">Frontend Code</h2>\n<p>The frontend code can be found in the src/main/webapp directory. eLinks uses a framework called JSF (JavaServer Faces), which allows pages to be written in HTML, and make calls to the backend Java code. The webapp directory contains the pages to be run, the resources they require, and also the required JSF configuration.</p>\n<p>The WEB-INTF subdirectory contains the configuration files. The important ones are:</p>\n<ul>\n<li>pretty-config.xml: This determines the pages available, and identifies any parameters available to the page. For example, the code below defines the link edit page. It will run the linkEditView class's load method when the page opens, and set the id parameter to be whatever is in the URL when navigating to the page. The page shown will be determined by the file /views/link-edit.xhtml.</li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><span class=\"nt\">&lt;url-mapping</span> <span class=\"na\">id=</span><span class=\"s\">&quot;link-edit&quot;</span><span class=\"nt\">&gt;</span>\n    <span class=\"nt\">&lt;pattern</span> <span class=\"na\">value=</span><span class=\"s\">&quot;/link/#{linkEditView.id}/edit&quot;</span> <span class=\"nt\">/&gt;</span>\n    <span class=\"nt\">&lt;view-id</span> <span class=\"na\">value=</span><span class=\"s\">&quot;/views/link-edit.xhtml&quot;</span> <span class=\"nt\">/&gt;</span>\n    <span class=\"nt\">&lt;action</span> <span class=\"na\">onPostback=</span><span class=\"s\">&quot;false&quot;</span><span class=\"nt\">&gt;</span>#{linkEditView.load}<span class=\"nt\">&lt;/action&gt;</span>\n<span class=\"nt\">&lt;/url-mapping&gt;</span>\n</pre></div>\n\n\n<ul>\n<li>web.xml: This contains most of the general config required to run the webapp. Most of this is unlikely to be changed, but you may need to add new listeners here if any other threads are added.</li>\n<li>faces-config.xml: This is used to register tools such as validators or converters with JSF.</li>\n</ul>\n<p>The bulk of the code is found in the templates and views directories. The templates give the overall layouts of the pages, to ensure consistency. The views give the specific content of each page. Some content is repeated between pages, such as the login button, which appears on every page. These can be found in views/partials.</p>\n<p>The resources directory contains all the other files needed by the webapp. This includes:</p>\n<ul>\n<li>Images, such as the logos, background, and loading animation.</li>\n<li>Sample licence files.</li>\n<li>CSS files: These determine the style of the webpage, such as setting font sizes and alignments of text.</li>\n<li>JavaScript files: These are simple scripts used by the site. They control features such as the show/hide button on the eData page.</li>\n</ul>\n<h2 id=\"backend-code\">Backend Code</h2>\n<p>The backend java code can be found in src/main/java/uk/ac/rl/aggregator. (Previously, the project was known as the aggregator, since it aggregates data and publications, but was rebranded as eLinks later in development. Ideally, we'd change the directory name, but it's probably not worth the trouble.)</p>\n<p>Note that Taiga automatically creates links when it detects certain extensions, such as .java. Please ignore these.</p>\n<p>The code is split into directories, as follows.</p>\n<ul>\n<li>\n<p><strong>Authority:</strong> The authority classes provide an interface between Solr (the search engine) and the rest of the webapp. Each source of data or publication handled by eLinks has it's own authority class, which are handled in similar ways.<br>\n<a href=\"http://Authority.java\" target=\"_blank\">Authority.java</a> provides a template for how each authority works, and an implementation of the template is provided for each source. Only one instance of each source authority will be running at any time, and the AuthorityFactory class is used to access that instance- for example, to get access to the ePubs authority, just call AuthorityFactory.createAuthority with an ePubs work as the argument, and the instance of the ePubs authority class will be returned.</p>\n<p>For more details on how this works, see the 'singleton' and 'factory method' pages on <a href=\"http://www.oodesign.com/\" target=\"_blank\">this</a> tutorial site.</p>\n</li>\n<li>\n<p><strong>Config:</strong> This allows the webapp to access external configuration files. Config.java reads the file /opt/elinks/config/<a href=\"http://config.properties\" target=\"_blank\">config.properties</a>, and provides access to the fields that have been set there. The fields themselves are detailed in <a href=\"http://Constants.java\" target=\"_blank\">Constants.java</a>. Since the names of the fields may change over time, it's much safer to refer to the fields by the name given in the constants class, rather than using the actual name in the config file.</p>\n</li>\n<li>\n<p><strong>Content:</strong> These modules relate to the help pages. They access the database to find help text, and allow users to edit and save the text on the pages.</p>\n</li>\n<li>\n<p><strong>Http:</strong> These are just a series of wrappers around web requests.</p>\n</li>\n<li>\n<p><strong>Indexer</strong>: The indexer classes are used to index new works and publications. When something new is added (for example, a new data item is manually linked to an ePubs record), the indexer needs to add the new data item to the search engine, but also update any related records with the new link. The indexer works similarly to the authorities, as each type of data has their own class which implements a template and is accessed by an IndexerFactory.<br>\n    In addition to the indexer classes, this directory contains the indexer thread. See <a class=\"reference wiki\" href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/elinks-multithreading\">here</a> for more details.</p>\n</li>\n<li>\n<p><strong>Search:</strong> These classes are used to search Solr, and manage the returned results.</p>\n</li>\n<li>\n<p><strong>Security:</strong> This directory contains the authentication and sanitiser classes. The authenticator simply checks if a username's password is valid, and the sanitiser is used to check the input of text fields is legitimate.</p>\n</li>\n<li>\n<p><strong>Services:</strong> This contains the DSpace URL linker, which searches ePubs for linked eData URLs which need a DOI to be added in. See <a class=\"reference wiki\" href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/elinks-multithreading\">here</a> for more details. <em>TODO: It might be better to move the other threads to this directory, and possibly rename it?</em></p>\n</li>\n<li>\n<p><strong>UI:</strong> This directory contains the bulk of the backing code. Certain classes have the line \"@ManagedBean(name = \"&lt;class name&gt;\")\" before the class definition- these are managed beans, which can be called from the HTML and provide the information the webapp needs to display.</p>\n<p>Many of the pages have a class to act as the managed bean, which wraps an implementation class containing all the actual functionality (for example, <a href=\"http://SearchView.java\" target=\"_blank\">SearchView.java</a> and <a href=\"http://SearchViewImpl.java\" target=\"_blank\">SearchViewImpl.java</a>). The reason for this distinction is that the implementation class can be passed to the unit tests. The Java unit tests don't actually run the webapp itself, so the managed bean infrastructure doesn't work there. By making the implementation of the class available separately to the managed bean, the implementation can be tested with JUnit.</p>\n<p><em>Note: this directory is probably a bit too big. In particular, the link manager could potentially be moved to a different directory, and the dspaceCreate subdirectory should probably be it's own directory.</em></p>\n</li>\n</ul>\n<h2 id=\"test-code\">Test Code</h2>\n<p>The backend code is tested by JUnit, and the frontend code is tested by Selenium. Both sets of tests are available in the src/test directory. See the links below for advice on testing:</p>\n<ul>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/testing-with-junit\">Testing with JUnit</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/testing-with-selenium\">Testing with Selenium</a></li>\n</ul>", "editions": 11, "version": 11}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109032, "slug": "elinks-demo", "content": "# How to demo ELINKS\n\nHow to approach a demo will vary depending on the aims of the demonstration. However, if you want to make sure you cover the core functionality of the tool, this is a good strategy to begin with. More general notes on running a demo are found [here](/project/alastairduncan-aggregator/wiki/demo-notes).\n\nA good example to use for manual linking is [doi:10.1021/acsnano.5b02623](http://dx.doi.org/10.1021/acsnano.5b02623) as the paper, and [doi:10.15125/BATH-00114](https://researchdata.bath.ac.uk/114/) as the dataset. The dataset title actually includes the name of the paper, so audiences won't be distracted by the science and will focus on your demo.\n\nFor publishing a new data item, use [doi:10.1107/S2052520615015176](http://dx.doi.org/10.1107/S2052520615015176) as the publication. Download the attached .dat files below, and upload those as the example. The files are just datasets, with analysis of three different Copperhydride samples.\n\nGenerally, it helps to split the demo into the following three stages:\n\n**First step:** Explain the overall aim of eLinks- providing a way for researchers to find and create links between publications and data.\n\n**Second step:** Explain searching for links.\n\n*   Use the search bar to find a sample publication which is already linked. As noted above, [doi:10.1021/acsnano.5b02623](http://dx.doi.org/10.1021/acsnano.5b02623) is a good choice, with search term 'nanoporous'.\n*   Show the page for the publication with linked data available: the left side shows metadata for the publication, right side has metadata for all linked datasets. Both sides have links to the DOI landing page for the publication or dataset.\n*   Show navigation between pages of associated data.\n\n**Third step:** Discuss creating new links. Break this down into two example problems- creating a link when the dataset is already published, and publishing the data and linking at the same time. For each of the two problems:\n\n*   Show the audience the paper and data you're linking together. Having a PDF open in another tab and showing the audience the paper helps to give context to the demo.\n*   Walk through the steps required to make the link. Keep things as simple as possible. If people want to see the optional fields, they will ask.\n*   Show the results, ideally including the effects outside of eLinks. If possible, show the ePubs page for the paper with the new DOI linked, and show the eData page of the new published item.\n*   You may also want to discuss the limitations of the demo. Usually, this is just the fact that a DOI won't be minted for the eData item.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-23T09:49:48Z", "modified_date": "2017-12-11T11:41:21Z", "html": "<h1 id=\"how-to-demo-elinks\">How to demo ELINKS</h1>\n<p>How to approach a demo will vary depending on the aims of the demonstration. However, if you want to make sure you cover the core functionality of the tool, this is a good strategy to begin with. More general notes on running a demo are found <a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/demo-notes\">here</a>.</p>\n<p>A good example to use for manual linking is <a href=\"http://dx.doi.org/10.1021/acsnano.5b02623\" target=\"_blank\">doi:10.1021/acsnano.5b02623</a> as the paper, and <a href=\"https://researchdata.bath.ac.uk/114/\" target=\"_blank\">doi:10.15125/BATH-00114</a> as the dataset. The dataset title actually includes the name of the paper, so audiences won't be distracted by the science and will focus on your demo.</p>\n<p>For publishing a new data item, use <a href=\"http://dx.doi.org/10.1107/S2052520615015176\" target=\"_blank\">doi:10.1107/S2052520615015176</a> as the publication. Download the attached .dat files below, and upload those as the example. The files are just datasets, with analysis of three different Copperhydride samples.</p>\n<p>Generally, it helps to split the demo into the following three stages:</p>\n<p><strong>First step:</strong> Explain the overall aim of eLinks- providing a way for researchers to find and create links between publications and data.</p>\n<p><strong>Second step:</strong> Explain searching for links.</p>\n<ul>\n<li>Use the search bar to find a sample publication which is already linked. As noted above, <a href=\"http://dx.doi.org/10.1021/acsnano.5b02623\" target=\"_blank\">doi:10.1021/acsnano.5b02623</a> is a good choice, with search term 'nanoporous'.</li>\n<li>Show the page for the publication with linked data available: the left side shows metadata for the publication, right side has metadata for all linked datasets. Both sides have links to the DOI landing page for the publication or dataset.</li>\n<li>Show navigation between pages of associated data.</li>\n</ul>\n<p><strong>Third step:</strong> Discuss creating new links. Break this down into two example problems- creating a link when the dataset is already published, and publishing the data and linking at the same time. For each of the two problems:</p>\n<ul>\n<li>Show the audience the paper and data you're linking together. Having a PDF open in another tab and showing the audience the paper helps to give context to the demo.</li>\n<li>Walk through the steps required to make the link. Keep things as simple as possible. If people want to see the optional fields, they will ask.</li>\n<li>Show the results, ideally including the effects outside of eLinks. If possible, show the ePubs page for the paper with the new DOI linked, and show the eData page of the new published item.</li>\n<li>You may also want to discuss the limitations of the demo. Usually, this is just the fact that a DOI won't be minted for the eData item.</li>\n</ul>", "editions": 25, "version": 23}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109033, "slug": "elinks-going-forward", "content": "# ELINKS: Going Forward\n\nHere is a list of ideas for ways in which the project could be improved.\n\n*   [Research Object Builder](/project/alastairduncan-aggregator/wiki/research-object-builder): A Tool that will aid a researcher with assembling a research object.\n\n*   [Document Builder:](/project/alastairduncan-aggregator/wiki/document-builder) Tools that will help a researcher create a document and publish the resources used within the document.\n\n*   [Research discovery tool](/project/alastairduncan-aggregator/wiki/research-discovery): Provide a way for users to quickly find potential links to data.\n\n*   Error handling: Currently, the linker just gives up if it can't index a record, even if it fails due to an HTTP request timing out. It would be ideal if the linker detected errors like this, and scheduled another attempt. There are a handful of issues which could complicated this, such as how many attempts to make before giving up on a record, and which errors warrant a second attempt.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-22T15:23:44Z", "modified_date": "2017-12-11T11:04:28Z", "html": "<h1 id=\"elinks-going-forward\">ELINKS: Going Forward</h1>\n<p>Here is a list of ideas for ways in which the project could be improved.</p>\n<ul>\n<li>\n<p><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/research-object-builder\">Research Object Builder</a>: A Tool that will aid a researcher with assembling a research object.</p>\n</li>\n<li>\n<p><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/document-builder\">Document Builder:</a> Tools that will help a researcher create a document and publish the resources used within the document.</p>\n</li>\n<li>\n<p><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/research-discovery\">Research discovery tool</a>: Provide a way for users to quickly find potential links to data.</p>\n</li>\n<li>\n<p>Error handling: Currently, the linker just gives up if it can't index a record, even if it fails due to an HTTP request timing out. It would be ideal if the linker detected errors like this, and scheduled another attempt. There are a handful of issues which could complicated this, such as how many attempts to make before giving up on a record, and which errors warrant a second attempt.</p>\n</li>\n</ul>", "editions": 4, "version": 4}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109034, "slug": "elinks-multithreading", "content": "# eLinks: Multithreading Issues\n\nThis page describes how the multithreading in eLinks works, and why it is required.\n\n## DSpace URL Linker\n\nWhen a new data item is published to eData, it takes up to 24 hours for the data item to have a DOI attached to the item. This is because the DOI minter for eData is only run periodically. As a result, when an item is published to eData, only the URL of the eData page is linked to the ePubs record. When the DOI is later available, we want that DOI to be linked to the ePubs record also.\n\nTo achieve this, the DSpace URL Linker thread will periodically check for ePubs record which have an eData URL linked but no corresponding eData DOI, then link the missing DOIs. The user can configure how frequently this happens by setting \"dspace.check.frequency\" to an appropriate number of seconds in /opt/elinks/config/config.properties.\n\n## Indexer Queue and Thread\n\nWhen a user manually links a new DOI to an item, the linker needs to contact whatever service is responsible for the data item to acquire metadata. This can take a lot of time, especially if the service is down, leading to the user being stuck on the loading page for a frustratingly long time before the request times out.\n\nTo avoid this, the linker keeps a queue of items needed to be indexed, and performs the actual indexing in a separate thread to the webapp. The thread is started whenever needed, such as in response to a user manually adding a link, and simply reindexes all items currently on the queue.\n\nAnother potential advantage of this system is that it has the potential to be used for error handling- if a service is down, and the request times out, the record can just be re-added to the indexer queue, to be tried again at a later date. This has yet to be implemented, but should probably be investigated soon.\n\n## ePubs Update Thread\n\nThe index for the linker needs to be kept up to date with all ePubs records. Datasets are only added to the index when they are linked to a publication, but every publication needs to be available for the user to create links to.\n\nIn order to do this, the update thread runs periodically to find which ePubs records have been updated recently, and re-indexes those records. This allows any new records to be added to the index, but also will pick up any changes to metadata for a publication, and also picks up any links added directly through the ePubs web interface.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-11T09:16:24Z", "modified_date": "2017-12-11T09:16:24Z", "html": "<h1 id=\"elinks-multithreading-issues\">eLinks: Multithreading Issues</h1>\n<p>This page describes how the multithreading in eLinks works, and why it is required.</p>\n<h2 id=\"dspace-url-linker\">DSpace URL Linker</h2>\n<p>When a new data item is published to eData, it takes up to 24 hours for the data item to have a DOI attached to the item. This is because the DOI minter for eData is only run periodically. As a result, when an item is published to eData, only the URL of the eData page is linked to the ePubs record. When the DOI is later available, we want that DOI to be linked to the ePubs record also.</p>\n<p>To achieve this, the DSpace URL Linker thread will periodically check for ePubs record which have an eData URL linked but no corresponding eData DOI, then link the missing DOIs. The user can configure how frequently this happens by setting \"dspace.check.frequency\" to an appropriate number of seconds in /opt/elinks/config/config.properties.</p>\n<h2 id=\"indexer-queue-and-thread\">Indexer Queue and Thread</h2>\n<p>When a user manually links a new DOI to an item, the linker needs to contact whatever service is responsible for the data item to acquire metadata. This can take a lot of time, especially if the service is down, leading to the user being stuck on the loading page for a frustratingly long time before the request times out.</p>\n<p>To avoid this, the linker keeps a queue of items needed to be indexed, and performs the actual indexing in a separate thread to the webapp. The thread is started whenever needed, such as in response to a user manually adding a link, and simply reindexes all items currently on the queue.</p>\n<p>Another potential advantage of this system is that it has the potential to be used for error handling- if a service is down, and the request times out, the record can just be re-added to the indexer queue, to be tried again at a later date. This has yet to be implemented, but should probably be investigated soon.</p>\n<h2 id=\"epubs-update-thread\">ePubs Update Thread</h2>\n<p>The index for the linker needs to be kept up to date with all ePubs records. Datasets are only added to the index when they are linked to a publication, but every publication needs to be available for the user to create links to.</p>\n<p>In order to do this, the update thread runs periodically to find which ePubs records have been updated recently, and re-indexes those records. This allows any new records to be added to the index, but also will pick up any changes to metadata for a publication, and also picks up any links added directly through the ePubs web interface.</p>", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109035, "slug": "elinks-repository", "content": "", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-22T09:47:27Z", "modified_date": "2017-11-22T09:47:27Z", "html": "", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109036, "slug": "general-taiga", "content": "# Notes on Taiga\n\nTaiga is used to organise and keep track of tasks within the project. There are two types of tasks- users stories and issues.\n\nUser stories are the start of the process for any piece of work. This includes not only coding and testing, but also background work and project management tasks. They tend to be bigger tasks which cover a period of a few weeks- for example, creating a new page which allows uploads to eData. A user story is then split into a number of smaller tasks, such as designing the new page, coding the web interface, coding the backend, and then testing the page. Each subtask can be assigned to a different member of the team. User stories are stored under the backlog tab, on the left of this page.\n\nIssues are tasks which are found during development, but don't necessarily fit into any user story. If you find a bug or a problem while working on a different area, raise it as an issue so it can be dealt with at a later date. To make it easier to fix in the future, make sure you explain what the bug is and fully document how to reproduce the bug. Ideally, attach some output logs to the issue, so people working on it in the future can be sure they are seeing the same behaviour. When the bug is fixed, it is helpful to leave a comment explaining how the issue was resolved, in case a similar issue occurs later.\n\n## Problems with the Wiki\n\nThe wiki software has a really annoying feature, where it tries to automatically detect links. This is fine for something like www.google.com, but it also thinks that any file named example.java is a URL. So, whenever a file is named, there's a strong chance there will be a hyperlink on it, which is basically impossible to remove.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-22T09:38:15Z", "modified_date": "2017-12-11T14:46:26Z", "html": "<h1 id=\"notes-on-taiga\">Notes on Taiga</h1>\n<p>Taiga is used to organise and keep track of tasks within the project. There are two types of tasks- users stories and issues.</p>\n<p>User stories are the start of the process for any piece of work. This includes not only coding and testing, but also background work and project management tasks. They tend to be bigger tasks which cover a period of a few weeks- for example, creating a new page which allows uploads to eData. A user story is then split into a number of smaller tasks, such as designing the new page, coding the web interface, coding the backend, and then testing the page. Each subtask can be assigned to a different member of the team. User stories are stored under the backlog tab, on the left of this page.</p>\n<p>Issues are tasks which are found during development, but don't necessarily fit into any user story. If you find a bug or a problem while working on a different area, raise it as an issue so it can be dealt with at a later date. To make it easier to fix in the future, make sure you explain what the bug is and fully document how to reproduce the bug. Ideally, attach some output logs to the issue, so people working on it in the future can be sure they are seeing the same behaviour. When the bug is fixed, it is helpful to leave a comment explaining how the issue was resolved, in case a similar issue occurs later.</p>\n<h2 id=\"problems-with-the-wiki\">Problems with the Wiki</h2>\n<p>The wiki software has a really annoying feature, where it tries to automatically detect links. This is fine for something like <a href=\"http://www.google.com,\" target=\"_blank\">www.google.com,</a> but it also thinks that any file named example.java is a URL. So, whenever a file is named, there's a strong chance there will be a hyperlink on it, which is basically impossible to remove.</p>", "editions": 3, "version": 3}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109037, "slug": "home", "content": "# Linker Wiki\n\nThis is the combined wiki for the eLinks project and the associated Jupyter Notebook extension.\n\n## eLinks\n\n*   [Introduction to eLinks](/project/alastairduncan-aggregator/wiki/intro-to-elinks)\n*   [Architecture](/project/alastairduncan-aggregator/wiki/architecture)\n*   [eLinks repository](/project/alastairduncan-aggregator/wiki/code-repo)\n*   [How to setup](/project/alastairduncan-aggregator/wiki/setting-up-the-elinks-vm)\n*   [Notes on code structure](/project/alastairduncan-aggregator/wiki/elinks-code)\n*   [Testing with JUnit](testing-with-junit)\n*   [Testing with Selenium](testing-with-selenium)\n*   [Ideas for going forward](/project/alastairduncan-aggregator/wiki/elinks-going-forward)\n*   [How to demo](/project/alastairduncan-aggregator/wiki/elinks-demo)\n\n## Jupyter\n\n*   [Introduction to the Jupyter Extension](/project/alastairduncan-aggregator/wiki/intro-to-jupyter)\n*   [Jupyter repository](https://github.com/stfc/jupyter-linker-extension)\n*   [How to setup](/project/alastairduncan-aggregator/wiki/setting-up-jupyer)\n*   [Setting up Jupyterhub](/project/alastairduncan-aggregator/wiki/setting-up-jupyterhub)\n*   [Notes on code structure](/project/alastairduncan-aggregator/wiki/jupyter-code)\n*   [Ideas for going forward](/project/alastairduncan-aggregator/wiki/jupyter-going-forward)\n*   [How to demo](/project/alastairduncan-aggregator/wiki/jupyter-demo)\n\n## General\n\n*   [Notes on Taiga](/project/alastairduncan-aggregator/wiki/general-taiga)\n*   [Demo notes](/project/alastairduncan-aggregator/wiki/demo-notes)\n*   [SCD Coding Standards](/project/alastairduncan-aggregator/wiki/scd-coding-standards)\n*   [Motivation behind the project](/project/alastairduncan-aggregator/wiki/motivation)\n\n*  [Setup of Cloud VM with Jupyter demo](/project/alastairduncan-aggregator/wiki/daaas-setup)", "owner": 133564, "last_modifier": 133564, "created_date": "2016-05-06T08:52:06Z", "modified_date": "2018-08-21T12:03:52Z", "html": "<h1 id=\"linker-wiki\">Linker Wiki</h1>\n<p>This is the combined wiki for the eLinks project and the associated Jupyter Notebook extension.</p>\n<h2 id=\"elinks\">eLinks</h2>\n<ul>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/intro-to-elinks\">Introduction to eLinks</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/architecture\">Architecture</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/code-repo\">eLinks repository</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/setting-up-the-elinks-vm\">How to setup</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/elinks-code\">Notes on code structure</a></li>\n<li><a class=\"reference wiki\" href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/testing-with-junit\">Testing with JUnit</a></li>\n<li><a class=\"reference wiki\" href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/testing-with-selenium\">Testing with Selenium</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/elinks-going-forward\">Ideas for going forward</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/elinks-demo\">How to demo</a></li>\n</ul>\n<h2 id=\"jupyter\">Jupyter</h2>\n<ul>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/intro-to-jupyter\">Introduction to the Jupyter Extension</a></li>\n<li><a href=\"https://github.com/stfc/jupyter-linker-extension\" target=\"_blank\">Jupyter repository</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/setting-up-jupyer\">How to setup</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/setting-up-jupyterhub\">Setting up Jupyterhub</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-code\">Notes on code structure</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-going-forward\">Ideas for going forward</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-demo\">How to demo</a></li>\n</ul>\n<h2 id=\"general\">General</h2>\n<ul>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/general-taiga\">Notes on Taiga</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/demo-notes\">Demo notes</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/scd-coding-standards\">SCD Coding Standards</a></li>\n<li>\n<p><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/motivation\">Motivation behind the project</a></p>\n</li>\n<li>\n<p><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/daaas-setup\">Setup of Cloud VM with Jupyter demo</a></p>\n</li>\n</ul>", "editions": 20, "version": 20}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109038, "slug": "intro-to-elinks", "content": "# Introduction to eLinks\n\neLinks is a web tool being developed by SCD. It allows researchers to easily see and create links between their publications and published datasets. Each publication and dataset gets a page in eLinks, showing a summary of the metadata, and details any linked datasets or publications. Here is an example page for an ePubs record, which is linked to one datacite record, shown on the right side of the page:\n\n![](https://media.taiga.io/attachments/7/2/a/1/1ea2ef571189da769004be4c880e9f8ca1e033337dacb5a7da9e00be4b2b/example-screenshot.png \"Example screenshot of a page from the eLinks interface\")\n\nCurrently, eLinks uses ePubs as it's source of publications, which can be linked to datasets from ICAT, Datacite, and the STFC run data repository, eData. Users can create new links to their publications in two ways: by adding a link to an existing URI, or by creating and linking a new item in eData.\n\nHere is an example of the page used to link an existing URI:\n\n![](https://media.taiga.io/attachments/4/e/a/1/816b9baf84e989025409072967ff6fa0824329ac4dde08c6a43b47110ad2/manage-links.png \"Example screenshot of manual link addition\")\n\nThe user simply provides a list of linked URIs, and new links should appear immediately on the publication and linked datasets' pages.\n\nThe alternative to this is linking a dataset which doesn't have a DOI yet. The user can publish their data to eData, while simultaneously creating a link to the publication.\n\n![](https://media.taiga.io/attachments/4/7/3/0/cd76324d228d54fd42f74c348e4d89aac62cffa5408d76f7c33404c7ccf8/create-data-item.png \"Example screenshot of manual link addition\")\n\nUsers must provide, as a minimum, a title, abstract and the datafiles to upload. All other mandatory metadata is autofilled from the user's federal ID, used to log in to the page. A range of other optional metadata can be provided, including embargoes, adding additional authors, and attaching a terms of service file.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-22T09:47:08Z", "modified_date": "2017-12-11T06:30:25Z", "html": "<h1 id=\"introduction-to-elinks\">Introduction to eLinks</h1>\n<p>eLinks is a web tool being developed by SCD. It allows researchers to easily see and create links between their publications and published datasets. Each publication and dataset gets a page in eLinks, showing a summary of the metadata, and details any linked datasets or publications. Here is an example page for an ePubs record, which is linked to one datacite record, shown on the right side of the page:</p>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/7/2/a/1/1ea2ef571189da769004be4c880e9f8ca1e033337dacb5a7da9e00be4b2b/example-screenshot.png\"></p>\n<p>Currently, eLinks uses ePubs as it's source of publications, which can be linked to datasets from ICAT, Datacite, and the STFC run data repository, eData. Users can create new links to their publications in two ways: by adding a link to an existing URI, or by creating and linking a new item in eData.</p>\n<p>Here is an example of the page used to link an existing URI:</p>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/4/e/a/1/816b9baf84e989025409072967ff6fa0824329ac4dde08c6a43b47110ad2/manage-links.png\"></p>\n<p>The user simply provides a list of linked URIs, and new links should appear immediately on the publication and linked datasets' pages.</p>\n<p>The alternative to this is linking a dataset which doesn't have a DOI yet. The user can publish their data to eData, while simultaneously creating a link to the publication.</p>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/4/7/3/0/cd76324d228d54fd42f74c348e4d89aac62cffa5408d76f7c33404c7ccf8/create-data-item.png\"></p>\n<p>Users must provide, as a minimum, a title, abstract and the datafiles to upload. All other mandatory metadata is autofilled from the user's federal ID, used to log in to the page. A range of other optional metadata can be provided, including embargoes, adding additional authors, and attaching a terms of service file.</p>", "editions": 18, "version": 12}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109039, "slug": "intro-to-jupyter", "content": "# INTRO TO JUPYTER EXTENSION\n\nThe aim of the jupyter extension is to provide users with a means of publishing data with additional context through Jupyter notebooks.\n\nIn recent years, researchers have been expected to publish their data, alongside their results. A natural progression of this is for researchers to also make their data analysis available, to give more meaning to their published data. One possible way to do this is by encouraging the use of electronic notebooks.\n\nThe project is still at a fairly early, experimental phase. The tool is not something specifically requested by users- rather, it is something which is likely to become more relevant as attitudes towards research change, and may be adopted steadily by the research community. As a result, this is in many ways a proof of concept, showing what jupyter extensions could be capable of.\n\n## How the extension works\n\nHere is an example of jupyter running the notebook extension:\n\n![](https://media.taiga.io/attachments/4/5/9/0/1c11c0d398fc7ba548e7598673ee0a28e3e8e95cb9d3d2b2c8ae0341f475/extension1.png \"Example of jupyter running the extension\")\n\nThe extension changes the interface for the notebook in several ways. Firstly, it adds the STFC branding to the layout- this is the most obvious way to check if the extension is running correctly. The majority of the new functionality is added under the 'data' option. Note that many of these options are also available in the other menus, as well.\n\n![](https://media.taiga.io/attachments/0/5/f/6/9a6e0f5f92345faf680f7d37470766262f0a45222e9da227c4061a201ef6/extension2.png \"The data menu\")\n\nThis is how the options in the menu work:\n\n*   Add Cell References: This creates a toolbar on the currently selected cell. Users can then add references specific to the selected cell. The references are automatically added to the notebook's metadata, and can be used to generate a references cell at the bottom of the notebook.\n*   Generate References: This takes all the cell specific references added using the references toolbars, and generates a cell at the bottom of the notebook with all the references.\n*   Insert Dataplot Cell: This creates a new cell with a dataplot toolbar. The toolbar includes a GUI where users provide a dataset, and some labels for the graph. This is used to generate a python script, which uses mataplotlib to generate a dataplot.\n*   Insert Analysis Cell: This creates a new cell with an analysis toolbar. The idea behind the analysis toolbar is to provide users with a way to manually create their own GUI toolbar. The notebook creator defines a set of variables, and provides a python script which uses those variables. Users of the notebook will now be able to manipulate variables in the python script by setting the values in the GUI.\n*   Edit Current Cell: If the current cell is a dataplot or analysis cell, this opens the relevant toolbar. Note that this can also be done by double clicking on the cell.\n*   Manage Metadata: This opens a dialogue which allows users to specify the metadata for the notebook.\n*   Manage Associated Data: This opens a dialogue which allows users to specify which local files are associated with the notebook. These are automatically added as the user creates analysis and dataplot cells, but users are likely to want to have control over this.\n*   Import Data: This allows users to import files\u00a0 from eData or their local machine, and make them available to the notebook.\n*   Publish to eData: This opens a dialogue which allows users to publish the notebook and associated data to the eData repository. Before publishing, users will have to confirm the metadata and associated data is correct.\n\n## Useful Links\n\nOfficial documentation on extensions:\n\n[Distributing Extensions]([jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Distributing](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Distributing) Jupyter Extensions as Python Packages.html)\n\nExtending the Notebook\n\nOld tutorial (still useful probably):\n\n[carreau.gitbooks.io/jupyter-book/content/notebook-extensions.html](https://carreau.gitbooks.io/jupyter-book/content/notebook-extensions.html)\n\nOther extensions (learn by example):\n\nnbgrader\n\ndashboards", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-22T09:41:41Z", "modified_date": "2017-12-19T12:09:25Z", "html": "<h1 id=\"intro-to-jupyter-extension\">INTRO TO JUPYTER EXTENSION</h1>\n<p>The aim of the jupyter extension is to provide users with a means of publishing data with additional context through Jupyter notebooks.</p>\n<p>In recent years, researchers have been expected to publish their data, alongside their results. A natural progression of this is for researchers to also make their data analysis available, to give more meaning to their published data. One possible way to do this is by encouraging the use of electronic notebooks.</p>\n<p>The project is still at a fairly early, experimental phase. The tool is not something specifically requested by users- rather, it is something which is likely to become more relevant as attitudes towards research change, and may be adopted steadily by the research community. As a result, this is in many ways a proof of concept, showing what jupyter extensions could be capable of.</p>\n<h2 id=\"how-the-extension-works\">How the extension works</h2>\n<p>Here is an example of jupyter running the notebook extension:</p>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/4/5/9/0/1c11c0d398fc7ba548e7598673ee0a28e3e8e95cb9d3d2b2c8ae0341f475/extension1.png\"></p>\n<p>The extension changes the interface for the notebook in several ways. Firstly, it adds the STFC branding to the layout- this is the most obvious way to check if the extension is running correctly. The majority of the new functionality is added under the 'data' option. Note that many of these options are also available in the other menus, as well.</p>\n<p><img alt=\"\" src=\"https://media.taiga.io/attachments/0/5/f/6/9a6e0f5f92345faf680f7d37470766262f0a45222e9da227c4061a201ef6/extension2.png\"></p>\n<p>This is how the options in the menu work:</p>\n<ul>\n<li>Add Cell References: This creates a toolbar on the currently selected cell. Users can then add references specific to the selected cell. The references are automatically added to the notebook's metadata, and can be used to generate a references cell at the bottom of the notebook.</li>\n<li>Generate References: This takes all the cell specific references added using the references toolbars, and generates a cell at the bottom of the notebook with all the references.</li>\n<li>Insert Dataplot Cell: This creates a new cell with a dataplot toolbar. The toolbar includes a GUI where users provide a dataset, and some labels for the graph. This is used to generate a python script, which uses mataplotlib to generate a dataplot.</li>\n<li>Insert Analysis Cell: This creates a new cell with an analysis toolbar. The idea behind the analysis toolbar is to provide users with a way to manually create their own GUI toolbar. The notebook creator defines a set of variables, and provides a python script which uses those variables. Users of the notebook will now be able to manipulate variables in the python script by setting the values in the GUI.</li>\n<li>Edit Current Cell: If the current cell is a dataplot or analysis cell, this opens the relevant toolbar. Note that this can also be done by double clicking on the cell.</li>\n<li>Manage Metadata: This opens a dialogue which allows users to specify the metadata for the notebook.</li>\n<li>Manage Associated Data: This opens a dialogue which allows users to specify which local files are associated with the notebook. These are automatically added as the user creates analysis and dataplot cells, but users are likely to want to have control over this.</li>\n<li>Import Data: This allows users to import files\u00a0 from eData or their local machine, and make them available to the notebook.</li>\n<li>Publish to eData: This opens a dialogue which allows users to publish the notebook and associated data to the eData repository. Before publishing, users will have to confirm the metadata and associated data is correct.</li>\n</ul>\n<h2 id=\"useful-links\">Useful Links</h2>\n<p>Official documentation on extensions:</p>\n<p><a href=\"[jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Distributing](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Distributing) Jupyter Extensions as Python Packages.html\" target=\"_blank\">Distributing Extensions</a></p>\n<p>Extending the Notebook</p>\n<p>Old tutorial (still useful probably):</p>\n<p><a href=\"https://carreau.gitbooks.io/jupyter-book/content/notebook-extensions.html\" target=\"_blank\">carreau.gitbooks.io/jupyter-book/content/notebook-extensions.html</a></p>\n<p>Other extensions (learn by example):</p>\n<p>nbgrader</p>\n<p>dashboards</p>", "editions": 33, "version": 31}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109040, "slug": "jupyter-backend-code", "content": "# Jupyter Backend Code\n\nThe code for the backend of the extension is found in the serverextension subdirectory. The _init_.py module registers a set of functions defined in the modules in this directory, which can then be called from the javascript by passing the path the function was registered on.\n\nHere is an overview of the functionality the server extension provides.\n\n### ConfigHandler\n\nThis handler opens the config file provided by the user, and parses any required fields. The reason this is handled by the server extension is that Javascript doesn't have access to the server's file systems. Instead, it makes a request for the python to access the filesystem, which is more secure.\n\n### DownloadHandler\n\nThis handler is responsible for requesting data downloads from eData. Like the ConfigHandler, this is required because Javascript can't manage files. Instead, it just tells the Python backend which files are needed.\n\n### DSpaceHandler\n\nThe main job of this handler is to request communities from eData, for filling out the possible collections which can be committed to. There is also some old code in here, which covers the old method of submitting to directly to eData. This is no longer used, and has been replaced by the SWORDHandler.\n\n### LDAPHandler\n\nThis handlers deals with requests to the login server. This covers both searching the database of user names for the name associated with a federal ID or for the autocomplete, and also authenticating a username/password combination.\n\n### LocalImportHandler\n\nThis handler is responsible for importing files from the user's local machine to the notebook directory.\n\n### MiscHandlers\n\nThis module does nothing, and should really be removed.\n\n### SWORDHandler\n\nThis handler is responsible for submitting the data item to eData using the SWORD protocol. In order to submit a new item, the handler has to zip together all the files for submission, along with a mets.xml file, which describes how SWORD should handle the contents of the zip file. Building the mets.xml file is incredibly tricky- it requires a very specific set of fields to be set, and there is very little documentation available to help you. Debugging isn't too bad, since the eData logs can often pinpoint which field is incorrect, but finding what the right value to set is can be tough.\n\n### UploadBundleHandler\n\nThis handler is used when uploading a notebook to eData.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-18T15:54:46Z", "modified_date": "2017-12-19T11:57:39Z", "html": "<h1 id=\"jupyter-backend-code\">Jupyter Backend Code</h1>\n<p>The code for the backend of the extension is found in the serverextension subdirectory. The <em>init</em>.py module registers a set of functions defined in the modules in this directory, which can then be called from the javascript by passing the path the function was registered on.</p>\n<p>Here is an overview of the functionality the server extension provides.</p>\n<h3 id=\"confighandler\">ConfigHandler</h3>\n<p>This handler opens the config file provided by the user, and parses any required fields. The reason this is handled by the server extension is that Javascript doesn't have access to the server's file systems. Instead, it makes a request for the python to access the filesystem, which is more secure.</p>\n<h3 id=\"downloadhandler\">DownloadHandler</h3>\n<p>This handler is responsible for requesting data downloads from eData. Like the ConfigHandler, this is required because Javascript can't manage files. Instead, it just tells the Python backend which files are needed.</p>\n<h3 id=\"dspacehandler\">DSpaceHandler</h3>\n<p>The main job of this handler is to request communities from eData, for filling out the possible collections which can be committed to. There is also some old code in here, which covers the old method of submitting to directly to eData. This is no longer used, and has been replaced by the SWORDHandler.</p>\n<h3 id=\"ldaphandler\">LDAPHandler</h3>\n<p>This handlers deals with requests to the login server. This covers both searching the database of user names for the name associated with a federal ID or for the autocomplete, and also authenticating a username/password combination.</p>\n<h3 id=\"localimporthandler\">LocalImportHandler</h3>\n<p>This handler is responsible for importing files from the user's local machine to the notebook directory.</p>\n<h3 id=\"mischandlers\">MiscHandlers</h3>\n<p>This module does nothing, and should really be removed.</p>\n<h3 id=\"swordhandler\">SWORDHandler</h3>\n<p>This handler is responsible for submitting the data item to eData using the SWORD protocol. In order to submit a new item, the handler has to zip together all the files for submission, along with a mets.xml file, which describes how SWORD should handle the contents of the zip file. Building the mets.xml file is incredibly tricky- it requires a very specific set of fields to be set, and there is very little documentation available to help you. Debugging isn't too bad, since the eData logs can often pinpoint which field is incorrect, but finding what the right value to set is can be tough.</p>\n<h3 id=\"uploadbundlehandler\">UploadBundleHandler</h3>\n<p>This handler is used when uploading a notebook to eData.</p>", "editions": 3, "version": 3}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109041, "slug": "jupyter-code", "content": "# JUPYTER EXTENSION CODE\n\nThe linker extension code is available from github [here](github.com/stfc/jupyter-linker-extension).\n\n## Overview of package\n\nThe code can be divided roughly into three- the frontend code, the backend code, and installation.\n\n*   [Frontend](/project/alastairduncan-aggregator/wiki/jupyter-frontend-code)\n*   [Backend](/project/alastairduncan-aggregator/wiki/jupyter-backend-code)\n*   [Installation](/project/alastairduncan-aggregator/wiki/jupyter-installation-code)", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-07T11:47:35Z", "modified_date": "2017-12-11T15:53:55Z", "html": "<h1 id=\"jupyter-extension-code\">JUPYTER EXTENSION CODE</h1>\n<p>The linker extension code is available from github <a href=\"github.com/stfc/jupyter-linker-extension\" target=\"_blank\">here</a>.</p>\n<h2 id=\"overview-of-package\">Overview of package</h2>\n<p>The code can be divided roughly into three- the frontend code, the backend code, and installation.</p>\n<ul>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-frontend-code\">Frontend</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-backend-code\">Backend</a></li>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-installation-code\">Installation</a></li>\n</ul>", "editions": 3, "version": 3}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109042, "slug": "jupyter-demo", "content": "# How to demo The Jupyter Extension\n\nHow to approach a demo will vary depending on the aims of the demonstration. However, if you want to make sure you cover the core functionality of the tool, this is a good strategy to begin with. More general notes on running a demo are found [here](/project/alastairduncan-aggregator/wiki/demo-notes).\n\nA zip file is attached below, containing an example notebook and all the data files required to run it. In this example, the notebook is a whole paper. It's not an ideal example, and many researchers have struggled to see a use for the extension because of it. Finding a better example would probably improve the demo significantly, but that requires finding a researcher willing to co-operate on developing one.\n\nHere is an overall strategy for covering the core features of the extension. This will change significantly as the project progresses, so keep it up to date as the project changes.\n\n1.  **Outline the aims of the extension, and the current stage of the project.** At time of writing, the project is very much a proof of concept, with the general aim of using jupyter extensions to improve access to data.\n\n2.  **Make sure the audience are familiar with the concept of an electronic notebook.** Younger researchers are more likely to have used one before. You may also need to clarify that Jupyter is a spinoff of iPython, which is more commonly used.\u00a0 If they're completely unfamiliar with electronic notebooks, it's sufficient to explain that they produce documents which combine Python code, images and text.\n\n3.  **Explain the example you're using, and outline the aim of the demo.** Currently, the example is a paper written in Jupyter based on some Copperhydride samples. The aim is to publish the data from the samples, alongside the notebook, to provide extra context. Briefly show the graph in the example, which is generated by three data files.\n\n4.  **Show the publish button.** You may need to explain that the notebook has accessto any files local to where the notebook is currently running. Walk through the dialogue, emphasising the simplicity of the interface.\n\n5.  **Show the new data item in eData.** Click the link in the success message. Talk through the data files which are now available in eData. You might have to explain that eData is a new repository for STFC. Explain the advantage of publishing the data with the notebook- not only is the data available, you can also download the notebook itself, and reproduce any analysis in the notebook.\n\n6.  **Discuss extra possible functionality.** The core of the extension is being able to easily publish the data, so focus on that first. Use the second half of the presentation to discuss the more experimental features, such as the toolbars used to draw graphs. Again, focus on the fact that this is a proof of concept, unless you have a specific example that researchers will want to use.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-24T10:34:23Z", "modified_date": "2017-11-27T08:54:18Z", "html": "<h1 id=\"how-to-demo-the-jupyter-extension\">How to demo The Jupyter Extension</h1>\n<p>How to approach a demo will vary depending on the aims of the demonstration. However, if you want to make sure you cover the core functionality of the tool, this is a good strategy to begin with. More general notes on running a demo are found <a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/demo-notes\">here</a>.</p>\n<p>A zip file is attached below, containing an example notebook and all the data files required to run it. In this example, the notebook is a whole paper. It's not an ideal example, and many researchers have struggled to see a use for the extension because of it. Finding a better example would probably improve the demo significantly, but that requires finding a researcher willing to co-operate on developing one.</p>\n<p>Here is an overall strategy for covering the core features of the extension. This will change significantly as the project progresses, so keep it up to date as the project changes.</p>\n<ol>\n<li>\n<p><strong>Outline the aims of the extension, and the current stage of the project.</strong> At time of writing, the project is very much a proof of concept, with the general aim of using jupyter extensions to improve access to data.</p>\n</li>\n<li>\n<p><strong>Make sure the audience are familiar with the concept of an electronic notebook.</strong> Younger researchers are more likely to have used one before. You may also need to clarify that Jupyter is a spinoff of iPython, which is more commonly used.\u00a0 If they're completely unfamiliar with electronic notebooks, it's sufficient to explain that they produce documents which combine Python code, images and text.</p>\n</li>\n<li>\n<p><strong>Explain the example you're using, and outline the aim of the demo.</strong> Currently, the example is a paper written in Jupyter based on some Copperhydride samples. The aim is to publish the data from the samples, alongside the notebook, to provide extra context. Briefly show the graph in the example, which is generated by three data files.</p>\n</li>\n<li>\n<p><strong>Show the publish button.</strong> You may need to explain that the notebook has accessto any files local to where the notebook is currently running. Walk through the dialogue, emphasising the simplicity of the interface.</p>\n</li>\n<li>\n<p><strong>Show the new data item in eData.</strong> Click the link in the success message. Talk through the data files which are now available in eData. You might have to explain that eData is a new repository for STFC. Explain the advantage of publishing the data with the notebook- not only is the data available, you can also download the notebook itself, and reproduce any analysis in the notebook.</p>\n</li>\n<li>\n<p><strong>Discuss extra possible functionality.</strong> The core of the extension is being able to easily publish the data, so focus on that first. Use the second half of the presentation to discuss the more experimental features, such as the toolbars used to draw graphs. Again, focus on the fact that this is a proof of concept, unless you have a specific example that researchers will want to use.</p>\n</li>\n</ol>", "editions": 5, "version": 4}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109043, "slug": "jupyter-distribution", "content": "# Distribution of the Extension\n\nOne of the biggest open questions for the project is how the jupyter extension will be made available to users. Setting it up manually is incredibly difficult, so we need to find an easy and reliable way for users to run it. Here are the current ideas.\n\n*   **Run it on a central server:** Instead of giving each user their own version of the extension, run it on a server. This is a difficult approach, but has a lot of advantages. The ideal scenario is that the eData page for the published notebook would contain a URL which took users directly to their own running version of the notebook, with any of the required data available to the notebook. This is difficult for a number of reasons, so is unlikely to happen.\n*   **Bundle the extension with the ICAT VM:** This is the most likely method of distribution. Essentially, the ICAT team have virtual machines available for users to clone, which include a variety of pre-setup ICAT software. The idea is that Jupyter and the extension would be made available as part of this, so users could run their own local version of Jupyter and download notebooks from eData as required.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-27T09:54:34Z", "modified_date": "2017-11-27T09:54:34Z", "html": "<h1 id=\"distribution-of-the-extension\">Distribution of the Extension</h1>\n<p>One of the biggest open questions for the project is how the jupyter extension will be made available to users. Setting it up manually is incredibly difficult, so we need to find an easy and reliable way for users to run it. Here are the current ideas.</p>\n<ul>\n<li><strong>Run it on a central server:</strong> Instead of giving each user their own version of the extension, run it on a server. This is a difficult approach, but has a lot of advantages. The ideal scenario is that the eData page for the published notebook would contain a URL which took users directly to their own running version of the notebook, with any of the required data available to the notebook. This is difficult for a number of reasons, so is unlikely to happen.</li>\n<li><strong>Bundle the extension with the ICAT VM:</strong> This is the most likely method of distribution. Essentially, the ICAT team have virtual machines available for users to clone, which include a variety of pre-setup ICAT software. The idea is that Jupyter and the extension would be made available as part of this, so users could run their own local version of Jupyter and download notebooks from eData as required.</li>\n</ul>", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109044, "slug": "jupyter-frontend-code", "content": "# Jupyter Frontend Code\n\nThe frontend code for the Jupyter extension is all found in the nbextensions subdirectory.\n\nThe common directory contains a small amount of code used by all parts of the extension- for example, the code that adapts the notebook header to display the STFC logo.\n\nThe notebook directory has code which is specific to the new functionality added, such as the new toolbars and publishing dialogue.\n\nNote that there are two files which are automatically generated by the build process: linker_extension_common.js and linker_extension_notebook.js. Any changes to these modules will be overwritten next time you build, so make sure to only work on the source files which these modules are built from.\n\n## Common Code\n\nThe extra code added here is largely cosmetic. It replaces the standard jupyter header with an STFC branded and coloured header, and introduces a new button for returning to the notebook home which should always be shown.\n\nThis is done by the modify_common_html.js module. This is loaded by the common_index.js module, which is installed in webpack.config.js.\n\n## Notebook Code\n\nThis is where the bulk of the new functionality is found.\n\nLike the common_index.js module in the common code, the notebook extensions are mostly loaded by the notebook.js module. This module registers callback functions for the various new dialogues and toolbars the notebook can now open, while also initialising the notebook if it is being used for the first time.\n\nThere are six main modules loaded by notebook_index.js.\n\n### modify_notebook_html\n\nThis module, like modify_common_html.js, essentially just adds new buttons to the existing interfaces. It creates the 'data' menu which is used to access the extension features, and also replaces the export to PDF feature with our custom implementation. Whenever you make a new feature, you'll have to add a button in this module to be able to access it.\n\nEverything in this module has to be done the first time the notebook is loaded. As a result, the module only consists of one function, named 'load'. This is made available to the rest of the webapp by the line at the end of the module:\n\n```javascript\nmodule.exports = {load: load};\n```\n\n### add_metadata\n\nThe add_metadata modules define the interface which allows users to add metadata describing the notebook being published. This gets a whole subdirectory, since there's quite a lot going on here.\n\nThe main module is named add_metadata_dialogue.js. _(As a side note, this is a poorly thought out name for the most important module in a directory- just 'add_metadata.js' could be much clearer?)_ This exports two methods: load, and add_metadata.\n\nThe load method registers the add_metadata method as a callback with Jupyter, so Jupyter can call the method later, and also sets the elements with ids manage_metadata and manage_metadata_edit to call the add_metadata method when they are clicked.\n\n```javascript\n$(\"#manage_metadata\").click(function () {\n    add_metadata();\n})\n$(\"#manage_metadata_edit\").click(function () {\n    add_metadata();\n});;\n```\n\nThe add_metadata method creates the add metadata dialogue box. It calls the add_metadata_input_fields module to create the two pages of input form, and creates a <form> container for them. It then creates a dialogue box with this container as the body of the box, and determines behaviour of the next and previous buttons.\n\nThe content of the input forms are defined in add_metadata_input_fields. This module exports three different methods:\n\n*   create_forms: This method creates the input fields to be used in the dialogue box.\n*   validate_fields1 and validate_fields2: These methods check the contents of the input fields, and control whether the user can move to the next page.\n*   save_metadata: This method takes the contents of the input fields and saves it to the appropriate fields in the notebook's metadata.\n\nThe simple fields, such as the title of the notebook, are handled in add_metadata_input_fields.js. The more complex fields have their own modules.\n\n*   Date: The date field requires a number of dropdown menus to be defined, and also has some slightly longwinded handling to make sure a valid date is input.\n*   Licences: This requires a long dropdown menu to be defined, which would make the main module less readable.\n*   TOS: The terms of service module requires an input file.\n*   Department: This controls two linked fields: the department, and the repository. Each department can have multiple possible repositories in eData which a notebook could be published to. As a result, the repository options must be repopulated every time the user sets their department.\n*   Author: The author fields provide an autocomplete feature which looks up author names as a user types. For this to happen, the input field needs to be provided with several methods, including how to perform the search, and how to handle the user selecting a suggestion. Furthermore, the number of authors is variable, so this module also defines the interface which allows users to add additional authors.\n\n### associated_data\n\nThis module allows users to determine which files should be published alongside the notebook.\n\nThe associated_data module itself is fairly simple, because most of the complexity is handled by the local_data module. The same interface for selecting local files is used in a few different places, so the local_data module takes care of creating the tree of files, managing user interaction with the tree, and saving the selected set of files when the user is finished.\n\n### setup_toolbars\n\nThe extension provides a number of cell toolbars which can be used to add context to and control individual cells.\u00a0 Each toolbar is defined in its own module, some of which require extra helper modules. The modules are all found in the toolbars subdirectory.\n\nThe toolbars and the callbacks that control that are registered in setup_toolbars.js. Registering a new toolbar is slightly fiddly, but adding a new toolbar should be fairly simple if you just copy and paste the code for an existing toolbar.\n\n_The modular layout of the subdirectory is not well thought out here. I only noticed this while writing the wiki, and didn't have time to fix it. Originally, there was only one toolbar, and extras were added later: the current layout was the first one that happened to work, and was only intended as a proof of concept. Ideally, the insert and enable functions for the different toolbars would be in the modules for their corresponding toolbars._\n\n### publish_notebook\n\nThis is the dialogue used to publish the notebook and data. It collects together the add metadata and associated data pages, and adds a login and confirm screen at the end of the dialogue. When the user confirms their metadata is correct, it then passes the notebook metadata to the custom_contents module to do the upload.\n\n### generate_references\n\nThis is associated with the references toolbar. Using the toolbar, users can attach references to specific cells, and then use the generate references tool to create or update a references cell at the bottom of the notebook.\n\n### download_data\n\nThis creates a dialogue used to import files to the jupyter notebook. The notebook has access to all the files local to the directory where the notebook is running, so to use files from outside the directory, files need to be imported.\n\nIt uses a tab structure, to allow users to upload files from their own machine or to download files from eData. This consists of two parts- the tab list, and the tab content. The list has class 'nav nav-tabs', and is just a list of items with appropriate display text and the data-toggle attribute set to 'tab'. These items will automatically be formatted to create the tabs, which control the 'tab-content' div below. This is just a sequence of divs, which have the 'tab-pane' class, which are hidden or shown depending on whether the tab with the corresponding id is selected currently.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-11T16:07:11Z", "modified_date": "2017-12-18T15:43:00Z", "html": "<h1 id=\"jupyter-frontend-code\">Jupyter Frontend Code</h1>\n<p>The frontend code for the Jupyter extension is all found in the nbextensions subdirectory.</p>\n<p>The common directory contains a small amount of code used by all parts of the extension- for example, the code that adapts the notebook header to display the STFC logo.</p>\n<p>The notebook directory has code which is specific to the new functionality added, such as the new toolbars and publishing dialogue.</p>\n<p>Note that there are two files which are automatically generated by the build process: linker_extension_common.js and linker_extension_notebook.js. Any changes to these modules will be overwritten next time you build, so make sure to only work on the source files which these modules are built from.</p>\n<h2 id=\"common-code\">Common Code</h2>\n<p>The extra code added here is largely cosmetic. It replaces the standard jupyter header with an STFC branded and coloured header, and introduces a new button for returning to the notebook home which should always be shown.</p>\n<p>This is done by the modify_common_html.js module. This is loaded by the common_index.js module, which is installed in webpack.config.js.</p>\n<h2 id=\"notebook-code\">Notebook Code</h2>\n<p>This is where the bulk of the new functionality is found.</p>\n<p>Like the common_index.js module in the common code, the notebook extensions are mostly loaded by the notebook.js module. This module registers callback functions for the various new dialogues and toolbars the notebook can now open, while also initialising the notebook if it is being used for the first time.</p>\n<p>There are six main modules loaded by notebook_index.js.</p>\n<h3 id=\"modify_notebook_html\">modify_notebook_html</h3>\n<p>This module, like modify_common_html.js, essentially just adds new buttons to the existing interfaces. It creates the 'data' menu which is used to access the extension features, and also replaces the export to PDF feature with our custom implementation. Whenever you make a new feature, you'll have to add a button in this module to be able to access it.</p>\n<p>Everything in this module has to be done the first time the notebook is loaded. As a result, the module only consists of one function, named 'load'. This is made available to the rest of the webapp by the line at the end of the module:</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"nx\">module</span><span class=\"p\">.</span><span class=\"nx\">exports</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"nx\">load</span><span class=\"o\">:</span> <span class=\"nx\">load</span><span class=\"p\">};</span>\n</pre></div>\n\n\n<h3 id=\"add_metadata\">add_metadata</h3>\n<p>The add_metadata modules define the interface which allows users to add metadata describing the notebook being published. This gets a whole subdirectory, since there's quite a lot going on here.</p>\n<p>The main module is named add_metadata_dialogue.js. <em>(As a side note, this is a poorly thought out name for the most important module in a directory- just 'add_metadata.js' could be much clearer?)</em> This exports two methods: load, and add_metadata.</p>\n<p>The load method registers the add_metadata method as a callback with Jupyter, so Jupyter can call the method later, and also sets the elements with ids manage_metadata and manage_metadata_edit to call the add_metadata method when they are clicked.</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"nx\">$</span><span class=\"p\">(</span><span class=\"s2\">&quot;#manage_metadata&quot;</span><span class=\"p\">).</span><span class=\"nx\">click</span><span class=\"p\">(</span><span class=\"kd\">function</span> <span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"nx\">add_metadata</span><span class=\"p\">();</span>\n<span class=\"p\">})</span>\n<span class=\"nx\">$</span><span class=\"p\">(</span><span class=\"s2\">&quot;#manage_metadata_edit&quot;</span><span class=\"p\">).</span><span class=\"nx\">click</span><span class=\"p\">(</span><span class=\"kd\">function</span> <span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"nx\">add_metadata</span><span class=\"p\">();</span>\n<span class=\"p\">});;</span>\n</pre></div>\n\n\n<p>The add_metadata method creates the add metadata dialogue box. It calls the add_metadata_input_fields module to create the two pages of input form, and creates a &lt;form&gt; container for them. It then creates a dialogue box with this container as the body of the box, and determines behaviour of the next and previous buttons.</p>\n<p>The content of the input forms are defined in add_metadata_input_fields. This module exports three different methods:</p>\n<ul>\n<li>create_forms: This method creates the input fields to be used in the dialogue box.</li>\n<li>validate_fields1 and validate_fields2: These methods check the contents of the input fields, and control whether the user can move to the next page.</li>\n<li>save_metadata: This method takes the contents of the input fields and saves it to the appropriate fields in the notebook's metadata.</li>\n</ul>\n<p>The simple fields, such as the title of the notebook, are handled in add_metadata_input_fields.js. The more complex fields have their own modules.</p>\n<ul>\n<li>Date: The date field requires a number of dropdown menus to be defined, and also has some slightly longwinded handling to make sure a valid date is input.</li>\n<li>Licences: This requires a long dropdown menu to be defined, which would make the main module less readable.</li>\n<li>TOS: The terms of service module requires an input file.</li>\n<li>Department: This controls two linked fields: the department, and the repository. Each department can have multiple possible repositories in eData which a notebook could be published to. As a result, the repository options must be repopulated every time the user sets their department.</li>\n<li>Author: The author fields provide an autocomplete feature which looks up author names as a user types. For this to happen, the input field needs to be provided with several methods, including how to perform the search, and how to handle the user selecting a suggestion. Furthermore, the number of authors is variable, so this module also defines the interface which allows users to add additional authors.</li>\n</ul>\n<h3 id=\"associated_data\">associated_data</h3>\n<p>This module allows users to determine which files should be published alongside the notebook.</p>\n<p>The associated_data module itself is fairly simple, because most of the complexity is handled by the local_data module. The same interface for selecting local files is used in a few different places, so the local_data module takes care of creating the tree of files, managing user interaction with the tree, and saving the selected set of files when the user is finished.</p>\n<h3 id=\"setup_toolbars\">setup_toolbars</h3>\n<p>The extension provides a number of cell toolbars which can be used to add context to and control individual cells.\u00a0 Each toolbar is defined in its own module, some of which require extra helper modules. The modules are all found in the toolbars subdirectory.</p>\n<p>The toolbars and the callbacks that control that are registered in setup_toolbars.js. Registering a new toolbar is slightly fiddly, but adding a new toolbar should be fairly simple if you just copy and paste the code for an existing toolbar.</p>\n<p><em>The modular layout of the subdirectory is not well thought out here. I only noticed this while writing the wiki, and didn't have time to fix it. Originally, there was only one toolbar, and extras were added later: the current layout was the first one that happened to work, and was only intended as a proof of concept. Ideally, the insert and enable functions for the different toolbars would be in the modules for their corresponding toolbars.</em></p>\n<h3 id=\"publish_notebook\">publish_notebook</h3>\n<p>This is the dialogue used to publish the notebook and data. It collects together the add metadata and associated data pages, and adds a login and confirm screen at the end of the dialogue. When the user confirms their metadata is correct, it then passes the notebook metadata to the custom_contents module to do the upload.</p>\n<h3 id=\"generate_references\">generate_references</h3>\n<p>This is associated with the references toolbar. Using the toolbar, users can attach references to specific cells, and then use the generate references tool to create or update a references cell at the bottom of the notebook.</p>\n<h3 id=\"download_data\">download_data</h3>\n<p>This creates a dialogue used to import files to the jupyter notebook. The notebook has access to all the files local to the directory where the notebook is running, so to use files from outside the directory, files need to be imported.</p>\n<p>It uses a tab structure, to allow users to upload files from their own machine or to download files from eData. This consists of two parts- the tab list, and the tab content. The list has class 'nav nav-tabs', and is just a list of items with appropriate display text and the data-toggle attribute set to 'tab'. These items will automatically be formatted to create the tabs, which control the 'tab-content' div below. This is just a sequence of divs, which have the 'tab-pane' class, which are hidden or shown depending on whether the tab with the corresponding id is selected currently.</p>", "editions": 7, "version": 7}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109045, "slug": "jupyter-going-forward", "content": "# Jupyter Extension: Going Forward\n\nThis part of the project is still in an experimental/proof of concept phase, so there are no requirements from user groups, and lots of potential ways to improve the project. Use this page to collect together any ideas on how the project could be improved, or directions it could take.\n\n*   [Distribution of the extension](/project/alastairduncan-aggregator/wiki/jupyter-extension)\n*   **Automatic detection of datafiles:** Rather than making users select the associated datafiles, the extension could scan any python cells for datafiles and add them to the list. This could either be done automatically when the user first tries to publish the notebook, or potentially offered as a button on the associated data page for users to trigger manually.\n*   **Integration with ICAT:** One suggestion was that it could be useful to be able to pass an ICAT DOI on the import data page, and have that data available to the notebook. This already works for eData, but could be expanded to ICAT (and possibly other repositories?)\n*   **Integration with mantid/other python libraries:** To help get the project off the ground, it would be really useful to have an impressive/useful example based on real science.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-27T10:29:47Z", "modified_date": "2017-11-27T10:33:09Z", "html": "<h1 id=\"jupyter-extension-going-forward\">Jupyter Extension: Going Forward</h1>\n<p>This part of the project is still in an experimental/proof of concept phase, so there are no requirements from user groups, and lots of potential ways to improve the project. Use this page to collect together any ideas on how the project could be improved, or directions it could take.</p>\n<ul>\n<li><a href=\"https://tree.taiga.io/project/alastairduncan-aggregator/wiki/jupyter-extension\">Distribution of the extension</a></li>\n<li><strong>Automatic detection of datafiles:</strong> Rather than making users select the associated datafiles, the extension could scan any python cells for datafiles and add them to the list. This could either be done automatically when the user first tries to publish the notebook, or potentially offered as a button on the associated data page for users to trigger manually.</li>\n<li><strong>Integration with ICAT:</strong> One suggestion was that it could be useful to be able to pass an ICAT DOI on the import data page, and have that data available to the notebook. This already works for eData, but could be expanded to ICAT (and possibly other repositories?)</li>\n<li><strong>Integration with mantid/other python libraries:</strong> To help get the project off the ground, it would be really useful to have an impressive/useful example based on real science.</li>\n</ul>", "editions": 2, "version": 2}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109046, "slug": "jupyter-installation-code", "content": "# Jupyter Installation Code\n\nHere is an overview of the code required to install the jupyter extension.\n\n### [setup.py](http://setup.py)\n\n[setup.py](http://setup.py) is used by python/pip to tell them what to do when trying to build and install the extension. This is used by every python package and isn't exclusive to a Jupyter extension.\n\nIn [setup.py](http://setup.py), there are custom 'sdist' commands that are used to build the extension into a zip file. We use this step to package up the javascript into a couple of files, and if running production mode - minimise the javascript files, using 'webpack'.\n\nWe also add some custom commands for user convinience. 'installextensions' and 'uninstallextensions' merely do the three steps required to install our extension: enable the server extension, then install and enable the javascript extension.\n\nEnabling the extensions merely populates an JSON file telling the notebook to attempt to load an extension with a specific name. Running 'jupyter --paths' can tell you the various paths on your system Jupyter uses to put stuff. The JSON files are classed as config files and so will be in one of the config directories. Installing a javascript extension merely moves the files specified by 'jupyter_nbextension_paths' into one of the data file folders listed by 'jupyter --paths'.\n\nNext, our custom install command moves some template files required by 'CustomNbconvertHandler'. This command is run on installation so any other things that need to happen on install can go here.\n\nFinally there are the 'setup_args'. A lot of these just give bits of metadata for the package but 'package_data' specifies non-python files to be added to the outputted package, 'packages' tell it where the python files are, 'install_requries' tells the package what other packages it depends on (so when a user installs it it can install the dependencies as well) and 'cmdclass' points it to the custom commands we created before.\n\n### package.json\n\nThis is where we tell node about our extension's node dependencies. There's two types of dependencies - a dev dependency is one required to build the package, and a regular dependency is one required to install it. We can automate filling these by specifying when installing via npm to save a package as a dependency. (via 'npm install --save' or 'npm install --save-dev')\n\n### webpack.config.js\n\nThis is what tells 'webpack' what to do. I don't know much about 'webpack' but this works. 'entry' basically tells 'webpack' the output file we want on the left, and the files used to build that file on the right. 'output' merely states where to put the output files. '[name]' refers the the left side of the 'entry' dict we specified before. 'loaders' ('module.rules' for'webpack 2' ) specifies the loaders 'webpack' should use when processing certain files. Mostly used for packaging css files with 'style-loader' and 'css-loader'. 'externals' maps variables inside the package with those outside. Basically acts as a replace I think? 'plugins' tells 'webpack' about plugins we are using. Used for 'es6-promise' but probably can't use for anything else.\n\nMostly, just play around with 'webpack' until it builds your javascript, then try it in the browser and see if it works in there. Lots of trial and error, but when it works it's really nice. You only really need to fiddle with it when adding external plugins and the like.\n\n### linker_extension/init.pi\n\n'linker_extension/' is our actual package folder. This is what is actually installed via 'pip' into python's 'dist-packages'. An '[init.py](http://init.py)' is required for python to be able to identify something as a package. So it is required, and we can also put useful code in there. In this one, we put in all the functions required by Jupyter to set up extensions. So, there's three types of extensions: a server extension, which adds custom handlers to the python tornado server the notebook runs; a notebook extension, which is a javascript extension for the front end of the notebook application; and more recently added a bundler extension, which allows us to more easily specify a custom download process.\n\n'_jupyter_server_extension_paths' tells Jupyter where the server extension is located. In our case, our extension is a module called 'serverextension' within the main 'linker_extension' package, so 'linker_extension.serverextension'\n\n'_jupyter_bundlerextension_paths' tell Jupyter about bundler extensions. Needs a list of dicts that specify a unique name, the actual label that will be displayed in the notebook on the button that uses this bundler, the name of the bundler extension modle (like 'serverextension', so 'linker_extension.bundlerextension') and the group (Download or Deploy)\n\n'jupyter_nbextension_paths' tells Jupyter where to find the notebook extensions. You must return a list of dictionaries, where each dictionary specifies an extension. 'section' defines what part of the notebook the extension runs on ('common' = every page, 'notebook','tree','edit' and 'terminal' are the choices, corresponding to different types of pages in the notebook app). 'src' gives the path, relative to the current directory, where to find the extension. 'dest' requires a path which is where Jupyter will dump the extension files since it copies them to an 'nbextension' directory in one of the data file locations). You'll need to preserve the path structure found in your original package, so usually just give the path 'linker_extension/[section]'. 'require' specifies the actual extension files needed by the extension. Since we package them all up into one javascript file, we just give the name of our file with it's full relative path 'linker_extension/[section]/linker_extension[section]'\n\n### linker_extension/jstest.pi\n\nPretty much copied from the 'notebook' repo. All it does differently is add in some test files to the directories so we can use them to test data upload. What it does is the same as what it does in the 'notebook' itself - it runs the Javascript test suite. See 'linker_extension/tests/README.nd' for details on how to run the tests.\n\n### linker_extension/nbextensions\n\nThis is where all the javascript files live. The index files '[section]_index.js' are the entry points for 'webpack' and import all the other files (via 'require' syntax'), run their loading functings and exports a function'load_ipython_extension' that is required by Jupyter. All the javascript files themselves should be documented inside.\n\n### linker_extension/resources\n\nMiscellaneous files needed by other files\n\n### linker_extension/bundlerextension/init.pi\n\nSpecifies the bundler extension. Pretty much copied from the nbconvert handler in the notebook.\n\n### linker_extension/serverextension/init.pi\n\ninit file for the server extension. Imports our custom handlers and creates route patterns for use within the notebook. Basically, if we send a request to '/ldap' we tell it to map that to our LDAPHandler.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-11T15:53:32Z", "modified_date": "2017-12-11T15:53:32Z", "html": "<h1 id=\"jupyter-installation-code\">Jupyter Installation Code</h1>\n<p>Here is an overview of the code required to install the jupyter extension.</p>\n<h3 id=\"setuppy\"><a href=\"http://setup.py\" target=\"_blank\">setup.py</a></h3>\n<p><a href=\"http://setup.py\" target=\"_blank\">setup.py</a> is used by python/pip to tell them what to do when trying to build and install the extension. This is used by every python package and isn't exclusive to a Jupyter extension.</p>\n<p>In <a href=\"http://setup.py\" target=\"_blank\">setup.py</a>, there are custom 'sdist' commands that are used to build the extension into a zip file. We use this step to package up the javascript into a couple of files, and if running production mode - minimise the javascript files, using 'webpack'.</p>\n<p>We also add some custom commands for user convinience. 'installextensions' and 'uninstallextensions' merely do the three steps required to install our extension: enable the server extension, then install and enable the javascript extension.</p>\n<p>Enabling the extensions merely populates an JSON file telling the notebook to attempt to load an extension with a specific name. Running 'jupyter --paths' can tell you the various paths on your system Jupyter uses to put stuff. The JSON files are classed as config files and so will be in one of the config directories. Installing a javascript extension merely moves the files specified by 'jupyter_nbextension_paths' into one of the data file folders listed by 'jupyter --paths'.</p>\n<p>Next, our custom install command moves some template files required by 'CustomNbconvertHandler'. This command is run on installation so any other things that need to happen on install can go here.</p>\n<p>Finally there are the 'setup_args'. A lot of these just give bits of metadata for the package but 'package_data' specifies non-python files to be added to the outputted package, 'packages' tell it where the python files are, 'install_requries' tells the package what other packages it depends on (so when a user installs it it can install the dependencies as well) and 'cmdclass' points it to the custom commands we created before.</p>\n<h3 id=\"packagejson\">package.json</h3>\n<p>This is where we tell node about our extension's node dependencies. There's two types of dependencies - a dev dependency is one required to build the package, and a regular dependency is one required to install it. We can automate filling these by specifying when installing via npm to save a package as a dependency. (via 'npm install --save' or 'npm install --save-dev')</p>\n<h3 id=\"webpackconfigjs\">webpack.config.js</h3>\n<p>This is what tells 'webpack' what to do. I don't know much about 'webpack' but this works. 'entry' basically tells 'webpack' the output file we want on the left, and the files used to build that file on the right. 'output' merely states where to put the output files. '[name]' refers the the left side of the 'entry' dict we specified before. 'loaders' ('module.rules' for'webpack 2' ) specifies the loaders 'webpack' should use when processing certain files. Mostly used for packaging css files with 'style-loader' and 'css-loader'. 'externals' maps variables inside the package with those outside. Basically acts as a replace I think? 'plugins' tells 'webpack' about plugins we are using. Used for 'es6-promise' but probably can't use for anything else.</p>\n<p>Mostly, just play around with 'webpack' until it builds your javascript, then try it in the browser and see if it works in there. Lots of trial and error, but when it works it's really nice. You only really need to fiddle with it when adding external plugins and the like.</p>\n<h3 id=\"linker_extensioninitpi\">linker_extension/init.pi</h3>\n<p>'linker_extension/' is our actual package folder. This is what is actually installed via 'pip' into python's 'dist-packages'. An '<a href=\"http://init.py\" target=\"_blank\">init.py</a>' is required for python to be able to identify something as a package. So it is required, and we can also put useful code in there. In this one, we put in all the functions required by Jupyter to set up extensions. So, there's three types of extensions: a server extension, which adds custom handlers to the python tornado server the notebook runs; a notebook extension, which is a javascript extension for the front end of the notebook application; and more recently added a bundler extension, which allows us to more easily specify a custom download process.</p>\n<p>'_jupyter_server_extension_paths' tells Jupyter where the server extension is located. In our case, our extension is a module called 'serverextension' within the main 'linker_extension' package, so 'linker_extension.serverextension'</p>\n<p>'_jupyter_bundlerextension_paths' tell Jupyter about bundler extensions. Needs a list of dicts that specify a unique name, the actual label that will be displayed in the notebook on the button that uses this bundler, the name of the bundler extension modle (like 'serverextension', so 'linker_extension.bundlerextension') and the group (Download or Deploy)</p>\n<p>'jupyter_nbextension_paths' tells Jupyter where to find the notebook extensions. You must return a list of dictionaries, where each dictionary specifies an extension. 'section' defines what part of the notebook the extension runs on ('common' = every page, 'notebook','tree','edit' and 'terminal' are the choices, corresponding to different types of pages in the notebook app). 'src' gives the path, relative to the current directory, where to find the extension. 'dest' requires a path which is where Jupyter will dump the extension files since it copies them to an 'nbextension' directory in one of the data file locations). You'll need to preserve the path structure found in your original package, so usually just give the path 'linker_extension/[section]'. 'require' specifies the actual extension files needed by the extension. Since we package them all up into one javascript file, we just give the name of our file with it's full relative path 'linker_extension/[section]/linker_extension[section]'</p>\n<h3 id=\"linker_extensionjstestpi\">linker_extension/jstest.pi</h3>\n<p>Pretty much copied from the 'notebook' repo. All it does differently is add in some test files to the directories so we can use them to test data upload. What it does is the same as what it does in the 'notebook' itself - it runs the Javascript test suite. See 'linker_extension/tests/README.nd' for details on how to run the tests.</p>\n<h3 id=\"linker_extensionnbextensions\">linker_extension/nbextensions</h3>\n<p>This is where all the javascript files live. The index files '[section]_index.js' are the entry points for 'webpack' and import all the other files (via 'require' syntax'), run their loading functings and exports a function'load_ipython_extension' that is required by Jupyter. All the javascript files themselves should be documented inside.</p>\n<h3 id=\"linker_extensionresources\">linker_extension/resources</h3>\n<p>Miscellaneous files needed by other files</p>\n<h3 id=\"linker_extensionbundlerextensioninitpi\">linker_extension/bundlerextension/init.pi</h3>\n<p>Specifies the bundler extension. Pretty much copied from the nbconvert handler in the notebook.</p>\n<h3 id=\"linker_extensionserverextensioninitpi\">linker_extension/serverextension/init.pi</h3>\n<p>init file for the server extension. Imports our custom handlers and creates route patterns for use within the notebook. Basically, if we send a request to '/ldap' we tell it to map that to our LDAPHandler.</p>", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109047, "slug": "motivation", "content": "# Motivation Behind the Project\n\nIn recent years, scientists have become expected to publish not only their results, but also the data behind the results. While the availability of data has increased, accessibility is still an issue.\n\nThe project didn't start with a specific set of requirements from users. Rather, it was an investigation of how published data could be made more accessible. The products being developed are in many ways intended to show how this can possibly be done, and may not be in production or fully utilized for several years. However, research is generally moving in a direction which means the need for them will grow over time.\n\nFor eLinks, the primary focus is on making data easier to find. It is hoped that researchers will use the tool to navigate between publications and datasets, as well as using it to publish datasets to eData and manually add links.\n\nThe Jupyter extension's aims are more ambitious. It encourages users to provide extra context to their data by pairing it with a notebook. The users will be able to see how the dataset was analysed, rather than just having access to the raw data, making the data more accessible. Ideally, the notebooks being published should provide context, even to a fairly non-expert researcher. This was the thinking behind the toolbars- if complex Python scripts can be wrapped by a graphical interface, then the notebooks can be shared more widely.\n\nAs an aside, both projects support eData by providing an additional means by which to publish data there.", "owner": 150610, "last_modifier": 133564, "created_date": "2017-11-22T11:37:36Z", "modified_date": "2017-12-05T15:44:08Z", "html": "<h1 id=\"motivation-behind-the-project\">Motivation Behind the Project</h1>\n<p>In recent years, scientists have become expected to publish not only their results, but also the data behind the results. While the availability of data has increased, accessibility is still an issue.</p>\n<p>The project didn't start with a specific set of requirements from users. Rather, it was an investigation of how published data could be made more accessible. The products being developed are in many ways intended to show how this can possibly be done, and may not be in production or fully utilized for several years. However, research is generally moving in a direction which means the need for them will grow over time.</p>\n<p>For eLinks, the primary focus is on making data easier to find. It is hoped that researchers will use the tool to navigate between publications and datasets, as well as using it to publish datasets to eData and manually add links.</p>\n<p>The Jupyter extension's aims are more ambitious. It encourages users to provide extra context to their data by pairing it with a notebook. The users will be able to see how the dataset was analysed, rather than just having access to the raw data, making the data more accessible. Ideally, the notebooks being published should provide context, even to a fairly non-expert researcher. This was the thinking behind the toolbars- if complex Python scripts can be wrapped by a graphical interface, then the notebooks can be shared more widely.</p>\n<p>As an aside, both projects support eData by providing an additional means by which to publish data there.</p>", "editions": 5, "version": 5}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109048, "slug": "research-discovery", "content": "# Research Discovery Tool\n\nThe main obstacle the tool needs to overcome is the initial population of the links. Once there are enough links in the tool for users to be able to find useful information, users will hopefully continue to add links to their own publications. However, getting the initial bulk of links could be a problem. Providing users with an easy way to quickly populate links could help get it off the ground.\n\nAn attempt at a solution was started, but never finished. The 'linkFind' page was intended to take the information for an ePubs record, and search Datacite/ICAT/eData for results which are considered likely to be related to the ePubs record. Users could then add a link to any related records by clicking one button, rather than searching for the DOI and adding it manually.\n\nObviously, the issue here is how to find the related data. One solution could be to search for records which share an author, or were published around the same time, or have similar keywords in the abstract and title. However, to make a meaningful attempt at this would require an example to work from- a fairly large set of papers, and the datasets which should be associated with them, so a set of sensible search terms could be formed from the example. This really shouldn't be too difficult to obtain, but would require more input from researchers than we currently get.\n\n## Anti-Plagarism Software\n\nA possible approach to discovery was to use the plagarism detection software CopyFind. The idea was for the tool to be run over some collection of ePubs records against a collection of data records, and any records which matched were considered to be possible links.\n\nAfter investigating the software, it was decided that this was not a good approach. The software would only run on windows, and when I looked into the source code, it seemed like the software wasn't actually doing very much that couldn't be just as easily achieved by searching the repositories manually.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-11T10:57:15Z", "modified_date": "2017-12-11T11:33:01Z", "html": "<h1 id=\"research-discovery-tool\">Research Discovery Tool</h1>\n<p>The main obstacle the tool needs to overcome is the initial population of the links. Once there are enough links in the tool for users to be able to find useful information, users will hopefully continue to add links to their own publications. However, getting the initial bulk of links could be a problem. Providing users with an easy way to quickly populate links could help get it off the ground.</p>\n<p>An attempt at a solution was started, but never finished. The 'linkFind' page was intended to take the information for an ePubs record, and search Datacite/ICAT/eData for results which are considered likely to be related to the ePubs record. Users could then add a link to any related records by clicking one button, rather than searching for the DOI and adding it manually.</p>\n<p>Obviously, the issue here is how to find the related data. One solution could be to search for records which share an author, or were published around the same time, or have similar keywords in the abstract and title. However, to make a meaningful attempt at this would require an example to work from- a fairly large set of papers, and the datasets which should be associated with them, so a set of sensible search terms could be formed from the example. This really shouldn't be too difficult to obtain, but would require more input from researchers than we currently get.</p>\n<h2 id=\"anti-plagarism-software\">Anti-Plagarism Software</h2>\n<p>A possible approach to discovery was to use the plagarism detection software CopyFind. The idea was for the tool to be run over some collection of ePubs records against a collection of data records, and any records which matched were considered to be possible links.</p>\n<p>After investigating the software, it was decided that this was not a good approach. The software would only run on windows, and when I looked into the source code, it seemed like the software wasn't actually doing very much that couldn't be just as easily achieved by searching the repositories manually.</p>", "editions": 2, "version": 2}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109049, "slug": "research-object-builder", "content": "# Research Object Builder #\nThis is a tool that enables a researcher to assemble resources that have been used to produce a publication into a research object from either items within repositories or from local disc. Items that have not been deposited in a permanent repository can be uploaded to a repository either as a bundle(zip, tgz etc.) and assigned a permanent identifier which can then be used as a reference. The tool should also be able to deposit the publication to a repository(probably as a preprint). This enables the researcher to follow their usual workflow.", "owner": 133564, "last_modifier": 133564, "created_date": "2016-05-06T09:17:50Z", "modified_date": "2016-05-06T09:17:50Z", "html": "<h1 id=\"research-object-builder\">Research Object Builder</h1>\n<p>This is a tool that enables a researcher to assemble resources that have been used to produce a publication into a research object from either items within repositories or from local disc. Items that have not been deposited in a permanent repository can be uploaded to a repository either as a bundle(zip, tgz etc.) and assigned a permanent identifier which can then be used as a reference. The tool should also be able to deposit the publication to a repository(probably as a preprint). This enables the researcher to follow their usual workflow.</p>", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109050, "slug": "scd-coding-standards", "content": "# Coding Standards\n\nThe department has a set of coding standards and guidelines available [here](https://github.com/stfc/SoftwareStandards/wiki). This is a work in progress, and more languages will be added over time.\n\nMuch of the eLinks and Jupyter extension code was written before these standards were set, so it may be inconsistently formatted in some places. Ideally, any new code written should follow the standards as closely as possible, but trying to be consistent with the rest of the code in the project should take priority.", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-22T10:50:52Z", "modified_date": "2017-11-22T10:50:52Z", "html": "<h1 id=\"coding-standards\">Coding Standards</h1>\n<p>The department has a set of coding standards and guidelines available <a href=\"https://github.com/stfc/SoftwareStandards/wiki\" target=\"_blank\">here</a>. This is a work in progress, and more languages will be added over time.</p>\n<p>Much of the eLinks and Jupyter extension code was written before these standards were set, so it may be inconsistently formatted in some places. Ideally, any new code written should follow the standards as closely as possible, but trying to be consistent with the rest of the code in the project should take priority.</p>", "editions": 1, "version": 1}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109051, "slug": "setting-up-jetty", "content": "# Jetty\n\nJetty is a piece of software which acts as a server for web applications. An application has two parts- a front end, written in HTML and CSS, and a backend, written in Java. The front end allows the website to be displayed in a browser, and provides a way for users to interact, and the backend is where information is processed.\n\n_**Note: When I tried to do this setup on a new virtualbox ubuntu machine, they didn't fully work. Alan or Alastair should be able to help fix these.**_\n\n## How to setup\n\nFirst, download Jetty and make the files available in the /opt directory. The example commands use wget, or you can manually download from [here](http://central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.4.7.v20170914/jetty-distribution-9.4.7.v20170914.tar.gz). You may need to find a more recent version, if the links are out of date- googling 'download jetty' should give you something appropriate as the top link.\n\n```bash\nsudo wget central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.4.7.v20170914/jetty-distribution-9.4.7.v20170914.tar.gz\nsudo tar zxvf jetty-distribution-9.4.7.v20170914.tar.gz -C /opt/\nsudo mv /opt/jetty-distribution-9.4.7.v20170914/ /opt/jetty\nsudo mkdir -p /opt/jetty/temp\n```\n\nNow create the jetty base directory. Principally, this will be used to store the webapps run by jetty, and store logs. An example directory is downloadable on the bottom of this page. Extract the tar file in the /opt directory:\n\n```bash\nsudo tar zxvf jetty-base.tar.gz -C /opt/\nsudo mkdir /var/log/jetty\nsudo ln -s /var/log/jetty /opt/jetty-base/logs\n```\n\nAdd a jetty user, and give them appropriate permissions and ownership of the right files.\n\n```bash\nsudo useradd --user-group --shell /bin/false --home-dir /opt/jetty/temp jetty\nsudo chown -R jetty:jetty /opt/jetty/\nsudo ln -s /opt/jetty/bin/jetty.sh /etc/init.d/jetty\nsudo chkconfig --add jetty\nsudo chkconfig --level 345 jetty on\nsudo chown -R jetty:jetty /opt/jetty-base/\nsudo chown -R jetty:jetty /var/log/jetty\n```\n\nUpdate the configuration of jetty.\n\n```bash\nsudo emacs /etc/default/jetty\nJETTY_HOME=/opt/jetty\nJETTY_BASE=/opt/jetty-base\nJETTY_USER=jetty\nJETTY_PORT=8080\nJETTY_HOST=192.168.12.10\nJAVA_OPTIONS=\"-Xms4096m -DAGGREGATOR_HOME=/opt/elinks\"\n```\n\nYou may also need to update java for this to run, depending on the version of jetty:\n\n```bash\nsudo yum -y install java-1.8.0-openjdk wget\n```\n\n## How to use\n\nUse the following commands to run jetty:\n\n```bash\nsudo service jetty start\nsudo service jetty restart\n```\n\nTo run a webapp through jetty, just move the .war file to the directory /opt/jetty-base/webapps. The app should be picked up automatically, and be found on a browser by going to http://<server name>:8080/index", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-21T15:26:21Z", "modified_date": "2017-12-19T13:39:06Z", "html": "<h1 id=\"jetty\">Jetty</h1>\n<p>Jetty is a piece of software which acts as a server for web applications. An application has two parts- a front end, written in HTML and CSS, and a backend, written in Java. The front end allows the website to be displayed in a browser, and provides a way for users to interact, and the backend is where information is processed.</p>\n<p><em><strong>Note: When I tried to do this setup on a new virtualbox ubuntu machine, they didn't fully work. Alan or Alastair should be able to help fix these.</strong></em></p>\n<h2 id=\"how-to-setup\">How to setup</h2>\n<p>First, download Jetty and make the files available in the /opt directory. The example commands use wget, or you can manually download from <a href=\"http://central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.4.7.v20170914/jetty-distribution-9.4.7.v20170914.tar.gz\" target=\"_blank\">here</a>. You may need to find a more recent version, if the links are out of date- googling 'download jetty' should give you something appropriate as the top link.</p>\n<div class=\"codehilite\"><pre><span></span>sudo wget central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.4.7.v20170914/jetty-distribution-9.4.7.v20170914.tar.gz\nsudo tar zxvf jetty-distribution-9.4.7.v20170914.tar.gz -C /opt/\nsudo mv /opt/jetty-distribution-9.4.7.v20170914/ /opt/jetty\nsudo mkdir -p /opt/jetty/temp\n</pre></div>\n\n\n<p>Now create the jetty base directory. Principally, this will be used to store the webapps run by jetty, and store logs. An example directory is downloadable on the bottom of this page. Extract the tar file in the /opt directory:</p>\n<div class=\"codehilite\"><pre><span></span>sudo tar zxvf jetty-base.tar.gz -C /opt/\nsudo mkdir /var/log/jetty\nsudo ln -s /var/log/jetty /opt/jetty-base/logs\n</pre></div>\n\n\n<p>Add a jetty user, and give them appropriate permissions and ownership of the right files.</p>\n<div class=\"codehilite\"><pre><span></span>sudo useradd --user-group --shell /bin/false --home-dir /opt/jetty/temp jetty\nsudo chown -R jetty:jetty /opt/jetty/\nsudo ln -s /opt/jetty/bin/jetty.sh /etc/init.d/jetty\nsudo chkconfig --add jetty\nsudo chkconfig --level <span class=\"m\">345</span> jetty on\nsudo chown -R jetty:jetty /opt/jetty-base/\nsudo chown -R jetty:jetty /var/log/jetty\n</pre></div>\n\n\n<p>Update the configuration of jetty.</p>\n<div class=\"codehilite\"><pre><span></span>sudo emacs /etc/default/jetty\n<span class=\"nv\">JETTY_HOME</span><span class=\"o\">=</span>/opt/jetty\n<span class=\"nv\">JETTY_BASE</span><span class=\"o\">=</span>/opt/jetty-base\n<span class=\"nv\">JETTY_USER</span><span class=\"o\">=</span>jetty\n<span class=\"nv\">JETTY_PORT</span><span class=\"o\">=</span><span class=\"m\">8080</span>\n<span class=\"nv\">JETTY_HOST</span><span class=\"o\">=</span><span class=\"m\">192</span>.168.12.10\n<span class=\"nv\">JAVA_OPTIONS</span><span class=\"o\">=</span><span class=\"s2\">&quot;-Xms4096m -DAGGREGATOR_HOME=/opt/elinks&quot;</span>\n</pre></div>\n\n\n<p>You may also need to update java for this to run, depending on the version of jetty:</p>\n<div class=\"codehilite\"><pre><span></span>sudo yum -y install java-1.8.0-openjdk wget\n</pre></div>\n\n\n<h2 id=\"how-to-use\">How to use</h2>\n<p>Use the following commands to run jetty:</p>\n<div class=\"codehilite\"><pre><span></span>sudo service jetty start\nsudo service jetty restart\n</pre></div>\n\n\n<p>To run a webapp through jetty, just move the .war file to the directory /opt/jetty-base/webapps. The app should be picked up automatically, and be found on a browser by going to <a href=\"http://&lt;server name>:8080/index\" target=\"_blank\">http://&lt;server name&gt;:8080/index</a></p>", "editions": 14, "version": 10}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109052, "slug": "setting-up-jupyer", "content": "# Jupyter Extension Setup\n\nSetting up the jupyter extension can be quite tricky. The process has several steps, and there is little feedback available to help with debugging if something goes wrong.\n\n## Setting up Jupyter\n\nThe first step is to get Jupyter running. Do this using pip, the Python package manager for Ubuntu.\n\n```bash\nsudo apt-get install python3-pip\nsudo pip3 install jupyter\n```\n\nTo test if the installation has worked, run the command 'jupyter notebook'. A browser window should open with the notebook running.\n\n## Installing the Extension\n\nFirst, install npm. This is a javascript package manager, required for building and deploying the frontend code. If this is done correctly, running 'which npm' should point you to a local file.\n\n```bash\nsudo apt-get install nodejs-legacy npm\n```\n\nThere are a handful of python packages you'll need. Install these using pip.\n\n```bash\nsudo pip3 install setuptools\nsudo pip3 install nbconvert\n```\n\nNow, clone the code from the git repository and build the extension. If this works, the required packages should appear in the dist subdirectory.\n\n```bash\ngit pull https://github.com/stfc/jupyter-linker-extension.git\ncd jupyter-linker-extension\nsudo -H python3 setup.py build_dev\n```\n\nFinally, the two following commands can be used to install and enable the extension.\n\n```bash\nsudo -H pip install dist/LinkerExtension-1.0.tar.gz\nsudo -H python3 setup.py installextensions\n```\n\nTo confirm this has worked, run a new jupyter notebook. The easiest way to tell if the installation has worked is that the STFC logo should replace the standard jupyter logo in the top left.\n\n## Rebuilding the Extension\n\nSometimes, you need to clean out all previously built files to get any changes picked up. I used this alias in my .bashrc to automate the rebuilding. You'll need to cd to your own workspace at the start, and possibly set the right version of Python during the cleanup.\n\n```bash\nalias jubu='cd ~/workspace/jupyter-linker-extension; sudo rm -r /usr/local/lib/python3.4/dist-packages/linker_extension/; sudo rm -r /usr/local/lib/python3.4/dist-packages/LinkerExtension-1.0.dist-info/; sudo -H python3 setup.py build_dev; sudo -H pip install dist/LinkerExtension-1.0.tar.gz; sudo -H python3 setup.py installextensions;'\n```", "owner": 150610, "last_modifier": 150610, "created_date": "2017-12-12T11:57:56Z", "modified_date": "2017-12-13T10:12:50Z", "html": "<h1 id=\"jupyter-extension-setup\">Jupyter Extension Setup</h1>\n<p>Setting up the jupyter extension can be quite tricky. The process has several steps, and there is little feedback available to help with debugging if something goes wrong.</p>\n<h2 id=\"setting-up-jupyter\">Setting up Jupyter</h2>\n<p>The first step is to get Jupyter running. Do this using pip, the Python package manager for Ubuntu.</p>\n<div class=\"codehilite\"><pre><span></span>sudo apt-get install python3-pip\nsudo pip3 install jupyter\n</pre></div>\n\n\n<p>To test if the installation has worked, run the command 'jupyter notebook'. A browser window should open with the notebook running.</p>\n<h2 id=\"installing-the-extension\">Installing the Extension</h2>\n<p>First, install npm. This is a javascript package manager, required for building and deploying the frontend code. If this is done correctly, running 'which npm' should point you to a local file.</p>\n<div class=\"codehilite\"><pre><span></span>sudo apt-get install nodejs-legacy npm\n</pre></div>\n\n\n<p>There are a handful of python packages you'll need. Install these using pip.</p>\n<div class=\"codehilite\"><pre><span></span>sudo pip3 install setuptools\nsudo pip3 install nbconvert\n</pre></div>\n\n\n<p>Now, clone the code from the git repository and build the extension. If this works, the required packages should appear in the dist subdirectory.</p>\n<div class=\"codehilite\"><pre><span></span>git pull https://github.com/stfc/jupyter-linker-extension.git\n<span class=\"nb\">cd</span> jupyter-linker-extension\nsudo -H python3 setup.py build_dev\n</pre></div>\n\n\n<p>Finally, the two following commands can be used to install and enable the extension.</p>\n<div class=\"codehilite\"><pre><span></span>sudo -H pip install dist/LinkerExtension-1.0.tar.gz\nsudo -H python3 setup.py installextensions\n</pre></div>\n\n\n<p>To confirm this has worked, run a new jupyter notebook. The easiest way to tell if the installation has worked is that the STFC logo should replace the standard jupyter logo in the top left.</p>\n<h2 id=\"rebuilding-the-extension\">Rebuilding the Extension</h2>\n<p>Sometimes, you need to clean out all previously built files to get any changes picked up. I used this alias in my .bashrc to automate the rebuilding. You'll need to cd to your own workspace at the start, and possibly set the right version of Python during the cleanup.</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"nb\">alias</span> <span class=\"nv\">jubu</span><span class=\"o\">=</span><span class=\"s1\">&#39;cd ~/workspace/jupyter-linker-extension; sudo rm -r /usr/local/lib/python3.4/dist-packages/linker_extension/; sudo rm -r /usr/local/lib/python3.4/dist-packages/LinkerExtension-1.0.dist-info/; sudo -H python3 setup.py build_dev; sudo -H pip install dist/LinkerExtension-1.0.tar.gz; sudo -H python3 setup.py installextensions;&#39;</span>\n</pre></div>", "editions": 5, "version": 5}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109053, "slug": "setting-up-jupyterhub", "content": "Important links:\n----------------\n\n\n[JupyterHub Github repository](https://github.com/jupyterhub/jupyterhub \"JupyterHub Github repository\")\n\n[Jupyter Notebook Github repository](https://github.com/jupyter/notebook \"Jupyter Notebook Github repository\")\n\n[JupyterHub Documentation](http://jupyterhub.readthedocs.io/en/latest/index.html \"JupyterHub Documentation\")\n\n[Getting Started with JupyterHub (Extra documentation)](http://jupyterhub-tutorial.readthedocs.io/en/latest/index.html \"Getting Started with JupyterHub\")\n\nInstallation\n------------\n\n\nFollow the instructions on the git repos to find out how to install both the notebook and JupyterHub. \n\nConfiguration\n-------------\n\n\nJupyter Notebook and JupyterHub should both work using their default configurations. Running \"jupyter notebook\" runs a single Jupyter Notebook server, and JupyterHub will set up a hub and a proxy. JupyterHub will need to be run as a priviledged user to be able to login as other users to create their notebook servers, so \"sudo jupyterhub\" is often more appropriate to run. JupyterHub will not run without SSL, so for testing purposes just run sudo jupyterhub --no-ssl until you set SSL up. (If you're using version >= 0.7 though they depreciated this testing option)\n\nThe bulk of configuration happens in the jupyterhub_config.py file. You can get JupyterHub to generate this file with the --generate-config option and it will create a file which details all the settings and their defaults. Most of these are not used so it is easier to just create a blank jupyterhub_config.py file and add in settings as you need them. Set `c = get_config()` at the top of the file.\n\nYou'll first want to get an SSL key and certificate so that you don't repeatedly have to use --no-ssl. A self-signed one if fine for testing. You need to place these files somewhere JupyterHub can find them. You can place them directly in the same directory as where you run JupyterHub or create a folder to run them from.\n\nNow, in jupyterhub_config.py, you'll need to set:\n```\nc.JupyterHub.ssl_cert = 'cert.pem'\nc.JupyterHub.ssl_key = 'key.key'\n```\nIn this case, cert.pem and key.key are in the same directory as where JupyterHub is run. Since we are going to be using https:// now, you can set the port to be 443 now with:\n```\nc.JupyterHub.port = 443\n```\nYou should now be able to run JupyterHub now without the --no-ssl option. Be sure to navigate to https:// link rather than to the http:// link the shell will provide you with.\n\nYou can create a list of admin users (these users can see what servers are running and stop other user's servers) using\n```\nc.Authenticator.admin_users = {'admin-username'}\n```\n\nYou can change the default logo file with \n```\nc.JupyterHub.logo_file = '/path/to/your/logo.png'\n```\n\nIf things seem to be going wrong with the proxy, check to see that there isn't a firewall blocking all th eports Jupyterhub wants to use. If this is the case, Jupyterhub requires 3 different ports to be allowed; `c.JupyterHub.port`, `c.JupyterHub.hub_port` and  `c.JupyterHub.proxy_api_port`. Ensure that these three settings are set to port numbers that arent blocked by the firewall.\n\n\nRunning with Docker\n-------------------\n\n\nWe've been using the default spawner up until now, but since we want to seperate users better and have mroe control over their individual servers we'll switch to using Docker and SingleUserSpawner. Install Docker (info [here](https://docs.docker.com/engine/installation/linux/ubuntulinux/ \"\")) and leave it alone as we do most our config in jupyterhub_config.py\n\nDownload DockerSpawner from [here](https://github.com/jupyterhub/dockerspawner \"\"). Follow the installation instructions in the readme.\n\nRun this command to download the correct docker image:\n\nsudo docker pull jupyter/singleuser\n\nNow since we're using SingleUserSpawner, we now need to set it up. Add the following lines to jupyterhub_config.py\n```\nc.JupyterHub.spawner_class = 'dockerspawner.SingleUserSpawner'\nc.DockerSpawner.container_image = 'jupyter/singleuser'\nc.JupyterHub.hub_ip = 'your computer/server's ip address here'\n```\n`c.DockerSpawner.remove_containers = True` (this line is mostly used for testing. You can opt to leave user containers)\n\nThis should allow you to spawn your user's notebook servers inside individual docker containers. To check this, you can type docker ps in a terminal to see the docker containers corresponding to the user servers.\n\nNow, we want to be able to control what damage the users can do to stay within their own docker container. For example, fork bombs. If you run a fork bomb in a notebook in the configuration we have now, the server will crash. To prevent this, we can add some arguments in jupyterhub_config.py that will be passed as arguments to Docker when it creates the containers.\n```\nc.DockerSpawner.extra_host_config = {\n\t'mem_limit': '1024m',\n\t'pids_limit': 100\n}\n```\nExtra config settings can be found [here](https://docker-py.readthedocs.io/en/latest/hostconfig/ \"\") and [here](https://docs.docker.com/engine/reference/run/ \"\")\n\nIn the current configuration, the user's files are contained in a temporary folder that would be lost forever if the container is deleted. In order to preserve user files we need to use docker volumes. These are folders that docker keeps track of that can be mounted to the relevant containers when spawned.\n```\nimport os\nnotebook_dir = os.environ.get('DOCKER_NOTEBOOK_DIR') or '/home/jovyan/work'\nc.DockerSpawner.notebook_dir = notebook_dir\nc.DockerSpawner.volumes = {\n        'jupyterhub-user-{username}': notebook_dir,\n}\n```\nThis creates a folder on the file system with the name template jupyterhub-user-{username} (so for example, my folder would be jupyterhub-user-mnf98541). This is then mounted to my container when it is created and set to be the notebook home directory inside the container.\n\nSETTING UP LDAPAUTHENTICATOR\n----------------------------\n\n\nSince STFC uses LDAP to authenticate users we can also use LDAP so that users can login with their federal ID and password. Download LDAPAuthenticator according to the instructions [here](https://github.com/jupyterhub/ldapauthenticator \"LDAPAuthenticator\"). The relevant config for STFC's LDAP is below.\n```\nc.JupyterHub.authenticator_class = 'ldapauthenticator.LDAPAuthenticator'\nc.LDAPAuthenticator.server_address = 'logon10.fed.cclrc.ac.uk'\nc.LDAPAuthenticator.bind_dn_template = [\n        'cn={username},ou=fbu,dc=fed,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=tbu,dc=fed,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=Swindon,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=obu,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=RALSpace,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=ROE,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=PPD,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=DLS,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=Facility Users,ou=fbu,dc=cclrc,dc=ac,dc=uk',\n        'cn={username},ou=RCaH,dc=cclrc,dc=ac,dc=uk'\n]\n```\n\nCustomising the hub and notebook\n--------------------------------\n\n\n### Jupyterhub\n\nThere's support for editing the css and some client-side javascript in both Jupyterhub and the Jupyter Notebook. Jupyterhub custom settings can be found by default in /usr/local/share/jupyter/hub/static/\n\nEditing the css as is is difficult due to it being compiled and minimised so one way to do it is to edit the files found in the jupyterhub git repo (under /share/jupyter/hub) and then run setup.py to create the necessary config files.\n\nSo, one can edit the LESS files and run python3 setup.py css to generate the css files. Then you can transfer the generated css files to the actual install location. Alternatively, style.min.css can be edited directly although this is harder sinc eit is minimisec.\n\nThere are also html files and javascript files that can be edited here to edit the html or javascript for the jupyterhub pages (but not the jupyter notebook servers themselves.) For example, in the page.html file in templates one can change the logo to link to https://www.stfc.ac.uk/ rather than the hub home page by replacing a href=\"{{base_url or logo_url}}\" wth a href=\"https://www.stfc.ac.uk/\"\n\n### Jupyter Notebook\n\nThe Jupyter Notebook has been extensively edited by me. However, in order to be able to use this edited version of the notebook in Jupyterhub required building a specialised Docker image for it and changing the container_image setting accordingly. A simple example Dockerfile is here:\n\n```\nFROM jupyterhub/singleuser\n\nUSER root\nCOPY /notebook-5.0.0.dev.tar.gz /home/jovyan/work\nRUN conda remove --quiet --yes notebook\nRUN conda install --quiet --yes ldap3\nRUN pip install notebook-5.0.0.dev.tar.gz\nRUN rm notebook-5.0.0.dev.tar.gz\n\nUSER jovyan\n```\n\nWhere notebook-5.0.0.dev.tar.gz is a python sdist created from my Jupyter Notebook using `python3 setup.py sdist`. This copies the sdist into the container, uninstalls the official version of the notebook, installs ldap3 (needed to search for authors in the metadata) and installs the new version of the notebook.", "owner": 163712, "last_modifier": 163712, "created_date": "2016-09-15T12:20:50Z", "modified_date": "2016-11-22T10:27:04Z", "html": "<h2 id=\"important-links\">Important links:</h2>\n<p><a href=\"https://github.com/jupyterhub/jupyterhub\" target=\"_blank\" title=\"JupyterHub Github repository\">JupyterHub Github repository</a></p>\n<p><a href=\"https://github.com/jupyter/notebook\" target=\"_blank\" title=\"Jupyter Notebook Github repository\">Jupyter Notebook Github repository</a></p>\n<p><a href=\"http://jupyterhub.readthedocs.io/en/latest/index.html\" target=\"_blank\" title=\"JupyterHub Documentation\">JupyterHub Documentation</a></p>\n<p><a href=\"http://jupyterhub-tutorial.readthedocs.io/en/latest/index.html\" target=\"_blank\" title=\"Getting Started with JupyterHub\">Getting Started with JupyterHub (Extra documentation)</a></p>\n<h2 id=\"installation\">Installation</h2>\n<p>Follow the instructions on the git repos to find out how to install both the notebook and JupyterHub. </p>\n<h2 id=\"configuration\">Configuration</h2>\n<p>Jupyter Notebook and JupyterHub should both work using their default configurations. Running \"jupyter notebook\" runs a single Jupyter Notebook server, and JupyterHub will set up a hub and a proxy. JupyterHub will need to be run as a priviledged user to be able to login as other users to create their notebook servers, so \"sudo jupyterhub\" is often more appropriate to run. JupyterHub will not run without SSL, so for testing purposes just run sudo jupyterhub --no-ssl until you set SSL up. (If you're using version &gt;= 0.7 though they depreciated this testing option)</p>\n<p>The bulk of configuration happens in the jupyterhub_config.py file. You can get JupyterHub to generate this file with the --generate-config option and it will create a file which details all the settings and their defaults. Most of these are not used so it is easier to just create a blank jupyterhub_config.py file and add in settings as you need them. Set <code>c = get_config()</code> at the top of the file.</p>\n<p>You'll first want to get an SSL key and certificate so that you don't repeatedly have to use --no-ssl. A self-signed one if fine for testing. You need to place these files somewhere JupyterHub can find them. You can place them directly in the same directory as where you run JupyterHub or create a folder to run them from.</p>\n<p>Now, in jupyterhub_config.py, you'll need to set:</p>\n<div class=\"codehilite\"><pre><span></span>c.JupyterHub.ssl_cert = &#39;cert.pem&#39;\nc.JupyterHub.ssl_key = &#39;key.key&#39;\n</pre></div>\n\n\n<p>In this case, cert.pem and key.key are in the same directory as where JupyterHub is run. Since we are going to be using https:// now, you can set the port to be 443 now with:</p>\n<div class=\"codehilite\"><pre><span></span>c.JupyterHub.port = 443\n</pre></div>\n\n\n<p>You should now be able to run JupyterHub now without the --no-ssl option. Be sure to navigate to https:// link rather than to the http:// link the shell will provide you with.</p>\n<p>You can create a list of admin users (these users can see what servers are running and stop other user's servers) using</p>\n<div class=\"codehilite\"><pre><span></span>c.Authenticator.admin_users = {&#39;admin-username&#39;}\n</pre></div>\n\n\n<p>You can change the default logo file with </p>\n<div class=\"codehilite\"><pre><span></span>c.JupyterHub.logo_file = &#39;/path/to/your/logo.png&#39;\n</pre></div>\n\n\n<p>If things seem to be going wrong with the proxy, check to see that there isn't a firewall blocking all th eports Jupyterhub wants to use. If this is the case, Jupyterhub requires 3 different ports to be allowed; <code>c.JupyterHub.port</code>, <code>c.JupyterHub.hub_port</code> and  <code>c.JupyterHub.proxy_api_port</code>. Ensure that these three settings are set to port numbers that arent blocked by the firewall.</p>\n<h2 id=\"running-with-docker\">Running with Docker</h2>\n<p>We've been using the default spawner up until now, but since we want to seperate users better and have mroe control over their individual servers we'll switch to using Docker and SingleUserSpawner. Install Docker (info <a href=\"https://docs.docker.com/engine/installation/linux/ubuntulinux/\" target=\"_blank\" title=\"\">here</a>) and leave it alone as we do most our config in jupyterhub_config.py</p>\n<p>Download DockerSpawner from <a href=\"https://github.com/jupyterhub/dockerspawner\" target=\"_blank\" title=\"\">here</a>. Follow the installation instructions in the readme.</p>\n<p>Run this command to download the correct docker image:</p>\n<p>sudo docker pull jupyter/singleuser</p>\n<p>Now since we're using SingleUserSpawner, we now need to set it up. Add the following lines to jupyterhub_config.py</p>\n<div class=\"codehilite\"><pre><span></span>c.JupyterHub.spawner_class = &#39;dockerspawner.SingleUserSpawner&#39;\nc.DockerSpawner.container_image = &#39;jupyter/singleuser&#39;\nc.JupyterHub.hub_ip = &#39;your computer/server&#39;s ip address here&#39;\n</pre></div>\n\n\n<p><code>c.DockerSpawner.remove_containers = True</code> (this line is mostly used for testing. You can opt to leave user containers)</p>\n<p>This should allow you to spawn your user's notebook servers inside individual docker containers. To check this, you can type docker ps in a terminal to see the docker containers corresponding to the user servers.</p>\n<p>Now, we want to be able to control what damage the users can do to stay within their own docker container. For example, fork bombs. If you run a fork bomb in a notebook in the configuration we have now, the server will crash. To prevent this, we can add some arguments in jupyterhub_config.py that will be passed as arguments to Docker when it creates the containers.</p>\n<div class=\"codehilite\"><pre><span></span>c.DockerSpawner.extra_host_config = {\n    &#39;mem_limit&#39;: &#39;1024m&#39;,\n    &#39;pids_limit&#39;: 100\n}\n</pre></div>\n\n\n<p>Extra config settings can be found <a href=\"https://docker-py.readthedocs.io/en/latest/hostconfig/\" target=\"_blank\" title=\"\">here</a> and <a href=\"https://docs.docker.com/engine/reference/run/\" target=\"_blank\" title=\"\">here</a></p>\n<p>In the current configuration, the user's files are contained in a temporary folder that would be lost forever if the container is deleted. In order to preserve user files we need to use docker volumes. These are folders that docker keeps track of that can be mounted to the relevant containers when spawned.</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"n\">notebook_dir</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;DOCKER_NOTEBOOK_DIR&#39;</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"s1\">&#39;/home/jovyan/work&#39;</span>\n<span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">DockerSpawner</span><span class=\"o\">.</span><span class=\"n\">notebook_dir</span> <span class=\"o\">=</span> <span class=\"n\">notebook_dir</span>\n<span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">DockerSpawner</span><span class=\"o\">.</span><span class=\"n\">volumes</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"s1\">&#39;jupyterhub-user-{username}&#39;</span><span class=\"p\">:</span> <span class=\"n\">notebook_dir</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</pre></div>\n\n\n<p>This creates a folder on the file system with the name template jupyterhub-user-{username} (so for example, my folder would be jupyterhub-user-mnf98541). This is then mounted to my container when it is created and set to be the notebook home directory inside the container.</p>\n<h2 id=\"setting-up-ldapauthenticator\">SETTING UP LDAPAUTHENTICATOR</h2>\n<p>Since STFC uses LDAP to authenticate users we can also use LDAP so that users can login with their federal ID and password. Download LDAPAuthenticator according to the instructions <a href=\"https://github.com/jupyterhub/ldapauthenticator\" target=\"_blank\" title=\"LDAPAuthenticator\">here</a>. The relevant config for STFC's LDAP is below.</p>\n<div class=\"codehilite\"><pre><span></span>c.JupyterHub.authenticator_class = &#39;ldapauthenticator.LDAPAuthenticator&#39;\nc.LDAPAuthenticator.server_address = &#39;logon10.fed.cclrc.ac.uk&#39;\nc.LDAPAuthenticator.bind_dn_template = [\n        &#39;cn={username},ou=fbu,dc=fed,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=tbu,dc=fed,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=Swindon,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=obu,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=RALSpace,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=ROE,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=PPD,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=DLS,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=Facility Users,ou=fbu,dc=cclrc,dc=ac,dc=uk&#39;,\n        &#39;cn={username},ou=RCaH,dc=cclrc,dc=ac,dc=uk&#39;\n]\n</pre></div>\n\n\n<h2 id=\"customising-the-hub-and-notebook\">Customising the hub and notebook</h2>\n<h3 id=\"jupyterhub\">Jupyterhub</h3>\n<p>There's support for editing the css and some client-side javascript in both Jupyterhub and the Jupyter Notebook. Jupyterhub custom settings can be found by default in /usr/local/share/jupyter/hub/static/</p>\n<p>Editing the css as is is difficult due to it being compiled and minimised so one way to do it is to edit the files found in the jupyterhub git repo (under /share/jupyter/hub) and then run setup.py to create the necessary config files.</p>\n<p>So, one can edit the LESS files and run python3 setup.py css to generate the css files. Then you can transfer the generated css files to the actual install location. Alternatively, style.min.css can be edited directly although this is harder sinc eit is minimisec.</p>\n<p>There are also html files and javascript files that can be edited here to edit the html or javascript for the jupyterhub pages (but not the jupyter notebook servers themselves.) For example, in the page.html file in templates one can change the logo to link to <a href=\"https://www.stfc.ac.uk/\" target=\"_blank\">https://www.stfc.ac.uk/</a> rather than the hub home page by replacing a href=\"{{base_url or logo_url}}\" wth a href=\"<a href=\"https://www.stfc.ac.uk/&quot;\" target=\"_blank\">https://www.stfc.ac.uk/\"</a></p>\n<h3 id=\"jupyter-notebook\">Jupyter Notebook</h3>\n<p>The Jupyter Notebook has been extensively edited by me. However, in order to be able to use this edited version of the notebook in Jupyterhub required building a specialised Docker image for it and changing the container_image setting accordingly. A simple example Dockerfile is here:</p>\n<div class=\"codehilite\"><pre><span></span>FROM jupyterhub/singleuser\n\nUSER root\nCOPY /notebook-5.0.0.dev.tar.gz /home/jovyan/work\nRUN conda remove --quiet --yes notebook\nRUN conda install --quiet --yes ldap3\nRUN pip install notebook-5.0.0.dev.tar.gz\nRUN rm notebook-5.0.0.dev.tar.gz\n\nUSER jovyan\n</pre></div>\n\n\n<p>Where notebook-5.0.0.dev.tar.gz is a python sdist created from my Jupyter Notebook using <code>python3 setup.py sdist</code>. This copies the sdist into the container, uninstalls the official version of the notebook, installs ldap3 (needed to search for authors in the metadata) and installs the new version of the notebook.</p>", "editions": 10, "version": 10}, {"project": 361447, "project_extra_info": {"name": "Linker", "slug": "alastairduncan-aggregator", "logo_small_url": null, "id": 361447}, "is_watcher": false, "total_watchers": 0, "id": 109054, "slug": "setting-up-salt", "content": "# Setting up the Linker: Salt\n\n## Installing Salt\n\nMost of the VM configuration is done via Salt. The first step is to install the 'salt-minion' process onto the VM:\n\n```\nyum install salt-minion\n```\n\nThe salt-minion must be configured, to recognise [shiloh.cse.rl.ac.uk](http://shiloh.cse.rl.ac.uk) as the salt master. Once this is done, the VM can be configured be the salt master. To do this, add the following line to /etc/salt/minion.\n\n```\nmaster: shiloh.cse.rl.ac.uk\n```\n\nNow, go to the salt master server, and create a configuration profile for elinks. This should be in the /srv/salt-config/salt directory. This involves creating a new elinks directory, and a file names init.sls. Initially, the profile is only required to install and set up apache on the minion, so the init.sls should be as follows:\n\n```\ninclude:\n- apache\n```\n\nNow, this has to be linked to the salt minion. To do this, add the following lines to the bottom of /srv/salt-config/salt/top.sls:\n\n```\n'elinks01.scd.rl.ac.uk':\n- elinks\n```\n\nBack on the salt minion, call the master to apply this config:\n\n```\nsudo salt-call state.apply\n```\n\nThis will get the new config from the salt master, and set up the VM as required.\n\nFinally, configure apache on the salt minion, by adding the following text to /etc/httpd/conf.d/elinks.conf:\n\n```\nServerName elinks01.scd.rl.ac.uk\n\nServerTokens Minor\nTraceEnable off\n\n<VirtualHost *:80>\nRedirect permanent / elinks01.scd.rl.ac.uk\n</VirtualHost>\n\n<VirtualHost *:443>\nSSLEngine on\n\nSSLCertificateFile /etc/pki/tls/certs/localhost.crt\nSSLCertificateKeyFile /etc/pki/tls/private/localhost.key\n\nProxyPass / localhost:8080\nProxyPassReverse / localhost:8080\n</VirtualHost>\n```", "owner": 150610, "last_modifier": 150610, "created_date": "2017-11-21T15:42:23Z", "modified_date": "2017-11-21T15:42:23Z", "html": "<h1 id=\"setting-up-the-linker-salt\">Setting up the Linker: Salt</h1>\n<h2 id=\"installing-salt\">Installing Salt</h2>\n<p>Most of the VM configuration is done via Salt. The first step is to install the 'salt-minion' process onto the VM:</p>\n<div class=\"codehilite\"><pre><span></span>yum install salt-minion\n</pre></div>\n\n\n<p>The salt-minion must be configured, to recognise <a href=\"http://shiloh.cse.rl.ac.uk\" target=\"_blank\">shiloh.cse.rl.ac.uk</a> as the salt master. Once this is done, the VM can be configured be the salt master. To do this, add the following line to /etc/salt/minion.</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"n\">master</span><span class=\"o\">:</span> <span class=\"n\">shiloh</span><span class=\"o\">.</span><span class=\"na\">cse</span><span class=\"o\">.</span><span class=\"na\">rl</span><span class=\"o\">.</span><span class=\"na\">ac</span><span class=\"o\">.</span><span class=\"na\">uk</span>\n</pre></div>\n\n\n<p>Now, go to the salt master server, and create a configuration profile for elinks. This should be in the /srv/salt-config/salt directory. This involves creating a new elinks directory, and a file names init.sls. Initially, the profile is only required to install and set up apache on the minion, so the init.sls should be as follows:</p>\n<div class=\"codehilite\"><pre><span></span>include:\n- apache\n</pre></div>\n\n\n<p>Now, this has to be linked to the salt minion. To do this, add the following lines to the bottom of /srv/salt-config/salt/top.sls:</p>\n<div class=\"codehilite\"><pre><span></span>&#39;elinks01.scd.rl.ac.uk&#39;:\n- elinks\n</pre></div>\n\n\n<p>Back on the salt minion, call the master to apply this config:</p>\n<div class=\"codehilite\"><pre><span></span>sudo salt-call state.apply\n</pre></div>\n\n\n<p>This will get the new config from the salt master, and set up the VM as required.</p>\n<p>Finally, configure apache on the salt minion, by adding the following text to /etc/httpd/conf.d/elinks.conf:</p>\n<div class=\"codehilite\"><pre><span></span>ServerName elinks01.scd.rl.ac.uk\n\nServerTokens Minor\nTraceEnable off\n\n<span class=\"nt\">&lt;VirtualHost</span> <span class=\"err\">*:80</span><span class=\"nt\">&gt;</span>\nRedirect permanent / elinks01.scd.rl.ac.uk\n<span class=\"nt\">&lt;/VirtualHost&gt;</span>\n\n<span class=\"nt\">&lt;VirtualHost</span> <span class=\"err\">*:443</span><span class=\"nt\">&gt;</span>\nSSLEngine on\n\nSSLCertificateFile /etc/pki/tls/certs/localhost.crt\nSSLCertificateKeyFile /etc/pki/tls/private/localhost.key\n\nProxyPass / localhost:8080\nProxyPassReverse / localhost:8080\n<span class=\"nt\">&lt;/VirtualHost&gt;</span>\n</pre></div>", "editions": 1, "version": 1}]